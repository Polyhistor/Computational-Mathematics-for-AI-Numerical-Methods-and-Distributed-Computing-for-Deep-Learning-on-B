\section{Background}

\subsection{Rationale}

Deep learning has revolutionized a wide spectrum of big data applications by providing state-of-the-art performance across domains. 
However, the increasing complexity of deep neural networks and the growing scale of datasets demand a deeper investigation into 
the computational methods that support such systems. Among these, numerical methods and distributed computing stand out as critical 
enablers for efficient model training and deployment at scale.

Numerical methods, particularly optimization algorithms, form the foundation for training deep neural networks, playing a key role 
in minimizing loss functions and ensuring convergence. As~\cite{najafabadi2015deep} emphasize, integrating deep learning with 
big data analytics introduces significant challenges, particularly in scalability and computational efficiency. 
This intersection calls for sophisticated mathematical tools to address issues related to high-dimensional data, model complexity, 
and computational constraints.

In parallel, distributed computing has emerged as an essential strategy for managing the large-scale computations involved in 
modern deep learning.~\cite{yan2023computational} highlights both the theoretical underpinnings and real-world implementations of such methods, 
underscoring the role of parallelism, GPU acceleration, federated learning, and other distributed paradigms. 
These techniques allow workloads to be distributed across clusters or edge devices, reducing training time and improving scalability.


Taken together, numerical optimization and distributed computing are not just complementary, they are interdependent components 
of scalable deep learning systems. Their combined use offers a path forward for developing more efficient, robust, and deployable 
AI solutions for big data scenarios.

This review synthesizes the current body of knowledge at the intersection of computational mathematics and scalable deep learning, 
providing a structured assessment of the most impactful methods in this space.

\subsection{Objectives}

The core objectives of this systematic literature review (SLR) are as follows:

\begin{enumerate}
    \item To identify and categorize cutting-edge numerical methods applied in deep learning for big data applications.
    \item To evaluate the effectiveness of distributed computing techniques in scaling deep learning systems.
    \item To compare these methods with respect to computational efficiency, scalability, and model accuracy.
    \item To uncover emerging trends and identify open research challenges at the convergence of numerical mathematics and distributed AI computing.
\end{enumerate}
