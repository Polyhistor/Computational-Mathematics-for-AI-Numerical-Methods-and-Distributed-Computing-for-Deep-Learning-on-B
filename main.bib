@article{kitchenham2002principles,
    author    = {Kitchenham, Barbara and Pfleeger, Shari Lawrence},
    journal   = {ACM SIGSOFT Software Engineering Notes},
    number    = {2},
    pages     = {20--24},
    publisher = {ACM},
    title     = {Principles of survey research: part 6: data analysis},
    volume    = {27},
    year      = {2002},
}


@book{fitch2001rand,
    abstractnote = {The RAND/UCLA appropriateness method (RAM) allows the abilty to see which part of the health care system is being over- or underused. This manual presents step-by-step guidelines for conceptualizing, designing and carrying out a study of the appropriateness of medical or surgical procedures},
    address      = {Santa Monica},
    author       = {Fitch, Kathryn and Bernstein, Steven J. and Aguilar, Mary Dolores},
    isbn         = {9781598752731},
    language     = {eng},
    publisher    = {RAND Corporation},
    title        = {The Rand/Ucla Appropriateness Method User’s Manual},
    year         = {2000},
}

@article{dalkey1969delphi,
    author   = {Dalkey, N.},
    doi      = {10.1016/S0016-3287(69)80025-X},
    issn     = {00163287},
    journal  = {Futures},
    language = {en},
    month    = sep,
    number   = {5},
    pages    = {408-426},
    rights   = {https://www.elsevier.com/tdm/userlicense/1.0/},
    title    = {An experimental study of group opinion},
    url      = {https://linkinghub.elsevier.com/retrieve/pii/S001632876980025X},
    volume   = {1},
    year     = {1969},
}

@article{diamond2014results,
    author    = {Diamond, Ian R and Grant, Robert C and Feldman, Brian M and Pencharz, Paul B and Ling, Simon C and Moore, Aideen M and Wales, Paul W},
    journal   = {Journal of Clinical Epidemiology},
    number    = {4},
    pages     = {402--409},
    publisher = {Elsevier},
    title     = {Results of a systematic review and meta-analysis of the presentations of Delphi studies},
    volume    = {67},
    year      = {2014},
}

@book{delbecq1975group,
    address    = {Glenview, Ill.},
    author     = {Delbecq, André L. and Van de Ven, Andrew H. and Gustafson, David H.},
    collection = {Management applications series},
    isbn       = {9780673075918},
    language   = {eng},
    publisher  = {Scott, Foresman and Co},
    series     = {Management applications series},
    title      = {Group techniques for program planning: a guide to nominal group and Delphi processes},
    year       = {1975},
}

@article{dalkey1963experimental,
    abstractnote = {This paper gives an account of an experiment in the use of the so-called DELPHI method, which was devised in order to obtain the most reliable opinion consensus of a group of experts by subjecting them to a series of questionnaires in depth interspersed with controlled opinion feedback.},
    author       = {Dalkey, Norman and Helmer, Olaf},
    doi          = {10.1287/mnsc.9.3.458},
    issn         = {0025-1909, 1526-5501},
    journal      = {Management Science},
    language     = {en},
    month        = apr,
    number       = {3},
    pages        = {458–467},
    title        = {An Experimental Application of the DELPHI Method to the Use of Experts},
    url          = {https://pubsonline.informs.org/doi/10.1287/mnsc.9.3.458},
    volume       = {9},
    year         = {1963},
}

@article{delbecq1971group,
    author    = {Delbecq, Andre L and Van de Ven, Andrew H},
    journal   = {Journal of Applied Behavioral Science},
    number    = {4},
    pages     = {466--492},
    publisher = {Sage Publications},
    title     = {A {G}roup {P}rocess {M}odel for {P}roblem {I}dentification and {P}rogram {P}lanning},
    volume    = {7},
    year      = {1971},
}

@techreport{laney2001data,
    added-at    = {2020-03-01T06:14:46.000+0100},
    author      = {Laney, Douglas},
    biburl      = {https://www.bibsonomy.org/bibtex/263868097d6e1998de3d88fcbb7670ca6/refshah},
    institution = {META Group},
    interhash   = {742811cb00b303261f79a98e9b80bf49},
    intrahash   = {63868097d6e1998de3d88fcbb7670ca6},
    keywords    = {big data},
    month       = {February},
    timestamp   = {2020-03-01T06:14:46.000+0100},
    title       = {{3D} Data Management: Controlling Data Volume, Velocity, and Variety},
    url         = {http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf},
    year        = 2001,
}

@book{krippendorff2004reliability,
    address   = {Thousand Oaks, CA},
    author    = {Krippendorff, Klaus},
    publisher = {Sage Publications},
    title     = {Content {A}nalysis: {A}n {I}ntroduction to {I}ts {M}ethodology},
    year      = {2004},
}

@article{petersen2008systematic,
    author  = {Petersen, Kai and Feldt, Robert and Mujtaba, Shahid and Mattsson, Michael},
    journal = {EASE},
    pages   = {68--77},
    title   = {Systematic mapping studies in software engineering},
    volume  = {8},
    year    = {2008},
}

@article{dean2012largemapreduce,
    author    = {Dean, Jeffrey and Ghemawat, Sanjay},
    journal   = {Communications of the ACM},
    number    = {1},
    pages     = {107--113},
    publisher = {ACM},
    title     = {MapReduce: simplified data processing on large clusters},
    volume    = {51},
    year      = {2008},
}

@inproceedings{li2014scaling,
    author    = {Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-Yiing},
    booktitle = {11th USENIX Symposium on Operating Systems Design and Implementation},
    pages     = {583--598},
    title     = {Scaling distributed machine learning with the parameter server},
    year      = {2014},
}

@article{zaharia2016apache,
    author    = {Zaharia, Matei and Xin, Reynold S and Wendell, Patrick and Das, Tathagata and Armbrust, Michael and Dave, Ankur and Meng, Xiangrui and Rosen, Josh and Venkataraman, Shivaram and Franklin, Michael J and others},
    journal   = {Communications of the ACM},
    number    = {11},
    pages     = {56--65},
    publisher = {ACM},
    title     = {Apache {S}park: {A} unified engine for big data processing},
    volume    = {59},
    year      = {2016},
}

@article{lecun2015deep,
    author    = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
    journal   = {Nature},
    number    = {7553},
    pages     = {436--444},
    publisher = {Nature Publishing Group},
    title     = {Deep learning},
    volume    = {521},
    year      = {2015},
}

@article{jordan2015machine,
    author    = {Jordan, Michael I and Mitchell, Tom M},
    journal   = {Science},
    number    = {6245},
    pages     = {255--260},
    publisher = {American Association for the Advancement of Science},
    title     = {Machine learning: Trends, perspectives, and prospects},
    volume    = {349},
    year      = {2015},
}

@book{goodfellow2016deep,
    address   = {Cambridge, MA},
    author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
    publisher = {MIT Press},
    title     = {Deep Learning},
    year      = {2016},
}


@article{moher2009preferred,
    author    = {Moher, David and Liberati, Alessandro and Tetzlaff, Jennifer and Altman, Douglas G and PRISMA Group},
    journal   = {PLoS medicine},
    number    = {7},
    pages     = {e1000097},
    publisher = {Public Library of Science},
    title     = {Preferred reporting items for systematic reviews and meta-analyses: the PRISMA statement},
    volume    = {6},
    year      = {2009},
}

@techreport{kitchenham2007guidelines,
    abstract             = {The objective of this report is to propose comprehensive guidelines for systematic literature reviews appropriate for software engineering researchers, including PhD students. A systematic literature review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guidelines presented in this report were derived from three existing guidelines used by medical researchers, two books produced by researchers with social science backgrounds and discussions with researchers from other disciplines who are involved in evidence-based practice. The guidelines have been adapted to reflect the specific problems of software engineering research. The guidelines cover three phases of a systematic literature review: planning the review, conducting the review and reporting the review. They provide a relatively high level description. They do not consider the impact of the research questions on the review procedures, nor do they specify in detail the mechanisms needed to perform meta-analysis.},
    added-at             = {2019-11-16T00:31:45.000+0100},
    author               = {Kitchenham, Barbara Ann and Charters, Stuart},
    biburl               = {https://www.bibsonomy.org/bibtex/23f4b30c0fe1435b642467af4cca120ef/jpmor},
    citeulike-article-id = {3955888},
    day                  = 09,
    institution          = {Keele University and Durham University Joint Report},
    interhash            = {aed0229656ada843d3e3f24e5e5c9eb9},
    intrahash            = {3f4b30c0fe1435b642467af4cca120ef},
    keywords             = {engineering evidence evidence-based literature real review software systematic},
    language             = {English},
    month                = {07},
    number               = {EBSE 2007-001},
    posted-at            = {2009-01-28 11:17:05},
    priority             = {2},
    school               = {Keele University},
    timestamp            = {2020-10-07T13:36:50.000+0200},
    title                = {Guidelines for performing Systematic Literature Reviews in Software Engineering},
    url                  = {https://www.elsevier.com/__data/promis_misc/525444systematicreviewsguide.pdf},
    year                 = 2007,
}


@article{cooper1988,
    author    = {Cooper, Harris M},
    journal   = {Knowledge in Society},
    number    = {1},
    pages     = {104--126},
    publisher = {Springer},
    title     = {Organizing knowledge syntheses: A taxonomy of literature reviews},
    volume    = {1},
    year      = {1988},
}

@article{Zhang202420230063,
    abstract  = {In order to give full play to the application of big data in film and television media and imaging in the cloud era, this study proposes a communication-efficient distributed deep neural network training method based on the DANE algorithm framework. The DANE algorithm is an approximate Newtonian method that has been widely used in communication-efficient distributed machine learning. It has the advantages of fast convergence and no need to calculate the inverse of the Hessian matrix, which can significantly reduce the communication and computational overhead in high-dimensional situations. In order to further improve the computational efficiency, it is necessary to study how to speed up the local optimization of DANE. It is a feasible method to choose to use the most popular adaptive gradient optimization algorithm Adam to replace the commonly used stochastic gradient descent method to solve the local single-machine suboptimization problem of DANE. Experiments show that Adam-based optimization can converge significantly faster than the original SGD-based implementation with little sacrifice in model generalization performance. With the increase of sampling rate, DANE-Adam significantly outperforms the DANE method in terms of convergence speed, and at the same time, the accuracy can be kept almost unchanged, which are 0.96, 0.88 and 0.75, respectively. This shows that Adam-based optimization can converge significantly faster than the original SGD-based implementation with little sacrifice in model generalization performance, with significant potential value.},
    author    = {Zhang, Qing},
    copyright = {http://creativecommons.org/licenses/by/4.0},
    doi       = {10.2478/amns.2023.1.00063},
    issn      = {2444-8656},
    journal   = {Applied Mathematics and Nonlinear Sciences},
    langid    = {english},
    month     = jan,
    number    = {1},
    pages     = {20230063},
    title     = {Analysis of the Application Methods of Film and Television Media and Images in the Era of Big Data Cloud},
    volume    = {9},
    year      = {2024},
}


@article{Chen2016331,
    author  = {Chen, Tao and Yang, Shichen and Li, Jianhua},
    doi     = {10.1109/TEVC.2015.2457491},
    journal = {IEEE Transactions on Evolutionary Computation},
    number  = {3},
    pages   = {331--343},
    title   = {Specialized Genetic Algorithm for Neural Network Architecture Optimization},
    volume  = {20},
    year    = {2016},
}

@inproceedings{dauphin2014identifying,
    abstract  = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
    address   = {Cambridge, MA, USA},
    author    = {Dauphin, Yann N. and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
    booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
    location  = {Montreal, Canada},
    numpages  = {9},
    pages     = {2933-2941},
    publisher = {MIT Press},
    series    = {NIPS'14},
    title     = {Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
    year      = {2014},
}

@article{schmidt2021descending,
    author  = {Schmidt, Robin M and Schneider, Frank and Hennig, Philipp},
    journal = {International Conference on Machine Learning},
    pages   = {9367--9376},
    title   = {Descending through a crowded valley—Benchmarking deep learning optimizers},
    year    = {2021},
}


@article{Gupta2021,
    author  = {Gupta, Harshit and Nath, Arun and Chakraborty, Sandip},
    doi     = {10.1109/MCOM.001.2000295},
    journal = {IEEE Communications Magazine},
    number  = {3},
    pages   = {98--104},
    title   = {Edge Intelligence for Smart Cities},
    volume  = {59},
    year    = {2021},
}


@article{Jin2021,
    author  = {Jin, Tianyu and Zhang, Haifeng and Liu, Song},
    doi     = {10.1016/j.ress.2021.107896},
    journal = {Reliability Engineering {\&} System Safety},
    pages   = {107896},
    title   = {Deep Learning for Industrial Fault Diagnosis},
    volume  = {215},
    year    = {2021},
}


@article{Fong2023,
    author  = {Fong, Simon and Deb, Suash and Wong, Raymond},
    doi     = {10.1109/TAI.2022.3209572},
    journal = {IEEE Transactions on Artificial Intelligence},
    number  = {2},
    pages   = {289--302},
    title   = {Adaptive Mutation Rates in Genetic Algorithms for Deep Learning},
    volume  = {4},
    year    = {2023},
}


@article{Ampavathi20211146,
    author     = {Ampavathi, Anusha and Saradhi, T. Vijaya},
    doi        = {10.1080/10255842.2020.1869726},
    issn       = {1025-5842, 1476-8259},
    journal    = {Computer Methods in Biomechanics and Biomedical Engineering},
    langid     = {english},
    month      = jul,
    number     = {10},
    pages      = {1146--1168},
    shorttitle = {Multi Disease-Prediction Framework Using Hybrid Deep Learning},
    title      = {Multi Disease-Prediction Framework Using Hybrid Deep Learning: An Optimal Prediction Model},
    volume     = {24},
    year       = {2021},
}


@article{Park2022,
    author  = {Park, Jongsoo and Yu, Minjia and Zhao, Tao},
    doi     = {10.1109/JSAC.2021.3118346},
    journal = {IEEE Journal on Selected Areas in Communications},
    number  = {1},
    pages   = {139--153},
    title   = {Adaptive Computation Techniques for Energy Efficiency in Deep Learning},
    volume  = {40},
    year    = {2022},
}


@article{Li20235058,
    abstract  = {With the rapid development of sensor technology, structural health monitoring data have tended to become more massive. Deep learning has advantages when handling big data, and has therefore been widely researched for diagnosing structural anomalies. However, for the diagnosis of different structural abnormalities, the model hyperparameters need to be adjusted according to different application scenarios, which is a complicated process. In this paper, a new strategy for building and optimizing 1D-CNN models is proposed that is suitable for diagnosing damage to different types of structure. This strategy involves optimizing hyperparameters with the Bayesian algorithm and improving model recognition accuracy using data fusion technology. Under the condition of sparse sensor measurement points, the entire structure is monitored, and the high-precision diagnosis of structural damage is performed. This method improves the applicability of the model to different structure detection scenarios, and avoids the shortcomings of traditional hyperparameter adjustment methods based on experience and subjectivity. In preliminary research on the simply supported beam test case, the efficient and accurate identification of parameter changes in small local elements was achieved. Furthermore, publicly available structural datasets were utilized to verify the robustness of the method, and a high identification accuracy rate of 99.85\% was achieved. Compared with other methods described in the literature, this strategy shows significant advantages in terms of sensor occupancy rate, computational cost, and identification accuracy.},
    author    = {Li, Xiaofei and Guo, Hainan and Xu, Langxing and Xing, Zezheng},
    copyright = {http://creativecommons.org/licenses/by/3.0/},
    doi       = {10.3390/s23115058},
    issn      = {1424-8220},
    journal   = {Sensors},
    keywords  = {1-D convolutional neural network,Bayesian optimization algorithm,decision-level fusion,structural anomaly detection,vibration signals},
    langid    = {english},
    month     = jan,
    number    = {11},
    pages     = {5058},
    publisher = {Multidisciplinary Digital Publishing Institute},
    title     = {Bayesian-{{Based Hyperparameter Optimization}} of {{1D-CNN}} for {{Structural Anomaly Detection}}},
    volume    = {23},
    year      = {2023},
}


@article{Zhu2021859,
    abstract   = {Ensuring the health and safety of senior citizens who live alone is a growing societal concern. The Activity of Daily Living (ADL) approach is a common means to monitor disease progression and the ability of these individuals to care for themselves. However, the prevailing sensor-based ADL monitoring systems primarily rely on wearable motion sensors, capture insufficient information for accurate ADL recognition, and do not provide a comprehensive understanding of ADLs at different granularities. Current healthcare IS and mobile analytics research focuses on studying the system, device, and provided services, and is in need of an end-to-end solution to comprehensively recognize ADLs based on mobile sensor data. This study adopts the design science paradigm and employs advanced deep learning algorithms to develop a novel hierarchical, multiphase ADL recognition framework to model ADLs at different granularities. We propose a novel 2D interaction kernel for convolutional neural networks to leverage interactions between human and object motion sensors. We rigorously evaluate each proposed module and the entire framework against state-of-the-art benchmarks (e.g., support vector machines, DeepConvLSTM, hidden Markov models, and topic-modeling-based ADLR) on two real-life motion sensor datasets that consist of ADLs at varying granularities: Opportunity and INTER. Results and a case study demonstrate that our framework can recognize ADLs at different levels more accurately. We discuss how stakeholders can further benefit from our proposed framework. Beyond demonstrating practical utility, we discuss contributions to the IS knowledge base for future design science-based cybersecurity, healthcare, and mobile analytics applications.},
    author     = {Zhu, Hongyi and Samtani, Sagar and Brown, Randall and Chen, Hsinchun},
    doi        = {10.25300/MISQ/2021/15574},
    issn       = {02767783, 21629730},
    journal    = {MIS Quarterly},
    month      = jun,
    number     = {2},
    pages      = {859--896},
    shorttitle = {A {{Deep Learning Approach}} for {{Recognizing Activity}} of {{Daily Living}} ({{ADL}}) for {{Senior Care}}},
    title      = {A {{Deep Learning Approach}} for {{Recognizing Activity}} of {{Daily Living}} ({{ADL}}) for {{Senior Care}}: {{Exploiting Interaction Dependency}} and {{Temporal Patterns}}},
    volume     = {45},
    year       = {2021},
}


@article{Zhao2020,
    author  = {Zhao, Liang and Wang, Jin and Li, Xiaohong},
    doi     = {10.1109/TNNLS.2019.2927703},
    journal = {IEEE Transactions on Neural Networks and Learning Systems},
    number  = {7},
    pages   = {2378--2389},
    title   = {Multi-Strategy Adaptive Methods for Deep Learning},
    volume  = {31},
    year    = {2020},
}


@article{Mao20242614,
    author    = {Mao, Shunan and Zhang, Shiliang},
    copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
    doi       = {10.1109/TIP.2024.3378461},
    issn      = {1057-7149, 1941-0042},
    journal   = {IEEE Transactions on Image Processing},
    pages     = {2614--2626},
    title     = {Robust {{Fine-Grained Visual Recognition With Neighbor-Attention Label Correction}}},
    volume    = {33},
    year      = {2024},
}


@article{Kanchanamala20232414IJACS,
    abstract = {Summary             In the present epoch of computing, the world has changed from older conventional print media to social platform channels. Fake news articles have the prospects to handle the opinions of the public and so may harm human groupings. Therefore, it is necessary to explore the authenticity and credibility of the news flash being shared on the internet community. Hence, this research paper devises an efficient and robust fake news detection model, named Exponential Chimp Optimization Algorithm (EChOA)-based Deep Neuro-Fuzzy Network (DNFN) for detecting fake news. The introduced model utilizes a MapReduce framework that includes the mapper and reducer phases for processing big data for detecting fake news. First phase of processing is the Mapper work, in which every input used in the database is processed and creates an intermediate key-value pair. In the reducer phase, the fusion of features is performed by arranging the features with the help of computing the optimal parameter and Rand similarity coefficient using a Deep Q Network (DQN). Here, the detection of fake news is obtained by DNFN, and the DNFN is done using implemented EChOA. The EChOA-based DNFN effectively generates robust and effective fake news detection performance by choosing the optimal feature subsets through feature fusion. The EChOA is designed by integrating the Exponential Weighted Moving Average (EWMA) and Chimp Optimization Algorithm (ChOA). Moreover, the EChOA-based DNFN method outperformed various former fake news detection approaches and attains the highest performance based on the testing accuracy is 0.909, sensitivity is 0.937, and specificity is 0.891 using the FakeNewsNet dataset.},
    author   = {Kanchanamala, Pendela and Selva Rani, B. and Vairamuthu, S.},
    doi      = {10.1002/acs.3645},
    issn     = {0890-6327, 1099-1115},
    journal  = {International Journal of Adaptive Control and Signal Processing},
    langid   = {english},
    month    = sep,
    number   = {9},
    pages    = {2414--2433},
    title    = {Exponential {{Chimp Optimization Algorithm}} Based {{Deep Neuro}}-{{Fuzzy Network}} with {{MapReduce}} Framework for Fake News Detection in Big Data Analytics},
    volume   = {37},
    year     = {2023},
}


@article{bottou2018optimization,
    author  = {Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
    journal = {SIAM Review},
    number  = {2},
    pages   = {223--311},
    title   = {Optimization methods for large-scale machine learning},
    volume  = {60},
    year    = {2018},
}


@article{Pal2023,
    author            = {Pal, Souvik and Jhanjhi, N.Z. and Abdulbaqi, Azmi Shawkat and Akila, D. and Alsubaei, Faisal S. and Almazroi, Abdulaleem Ali},
    doi               = {10.3390/su15065104},
    journal           = {Sustainability (Switzerland)},
    note              = {Cited by: 25; All Open Access, Gold Open Access},
    number            = {6},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {An Intelligent Task Scheduling Model for Hybrid Internet of Things and Cloud Environment for Big Data Applications},
    type              = {Article},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169085570&doi=10.3390%2fsu15065104&partnerID=40&md5=9956b585b61826605d8df1c0b65b85ce},
    volume            = {15},
    year              = {2023},
}


@article{Gao2024922,
    author  = {Gao, Ai and Xu, Shengnan and Zhao, Zichen and Shang, Haibin and Xu, Rui},
    doi     = {10.23919/JSEE.2024.000048},
    issn    = {1004-4132},
    journal = {Journal of Systems Engineering and Electronics},
    month   = aug,
    number  = {4},
    pages   = {922--931},
    title   = {Fault {{Diagnosis Method}} of {{Link Control System}} for {{Gravitational Wave Detection}}},
    volume  = {35},
    year    = {2024},
}


@article{Gujjeti2021241,
    author            = {Gujjeti, Sridhar and Pabboju, Suresh},
    doi               = {10.1007/978-981-16-0878-0_24},
    journal           = {Smart Innovation, Systems and Technologies},
    note              = {Cited by: 0},
    pages             = {241--250},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {Rider-Deep Belief Network-Based MapReduce Framework for Big Data Classification},
    type              = {Conference paper},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112226242&doi=10.1007%2f978-981-16-0878-0_24&partnerID=40&md5=70da5a142d6a74b6b071821029f829c9},
    volume            = {225},
    year              = {2021},
}


@article{Lima2024,
    author     = {Lima, Jos{\e} G. B. A. and Gomes, Anderson S. L. and {De Almeida-Filho}, Adiel T.},
    doi        = {10.1007/s11831-024-10163-x},
    issn       = {1134-3060, 1886-1784},
    journal    = {Archives of Computational Methods in Engineering},
    langid     = {english},
    month      = jul,
    shorttitle = {Intelligent {{Materials Improvement Through Artificial Intelligence Approaches}}},
    title      = {Intelligent {{Materials Improvement Through Artificial Intelligence Approaches}}: {{A Systematic Literature Review}}},
    year       = {2024},
}


@article{Zhao2022,
    author  = {Zhao, Wei and Liu, Jun and Chen, Zhuo},
    doi     = {10.1109/TMC.2021.3058267},
    journal = {IEEE Transactions on Mobile Computing},
    number  = {9},
    pages   = {3268--3283},
    title   = {Hardware-Specific Optimizations for Mobile GPU Acceleration},
    volume  = {21},
    year    = {2022},
}


@article{Singh2022,
    author  = {Singh, Rajveer and Kumar, Dinesh and Sharma, Vivek},
    doi     = {10.1109/TSG.2021.3134577},
    journal = {IEEE Transactions on Smart Grid},
    number  = {3},
    pages   = {2137--2148},
    title   = {Smart Grid Optimization using Deep Learning},
    volume  = {13},
    year    = {2022},
}


@article{Lee2021,
    author  = {Lee, Jaewon and Park, Jinyoung and Kim, Heeyoul},
    doi     = {10.1109/TITS.2020.3043030},
    journal = {IEEE Transactions on Intelligent Transportation Systems},
    number  = {7},
    pages   = {4141--4154},
    title   = {Deep Learning for Intelligent Transportation Systems},
    volume  = {22},
    year    = {2021},
}


@article{Liu20211735Conf,
    abstract   = {The growing attention on location-based services has promoted the development of indoor localization studies. Existing techniques mainly use Received Signal Strength Indicator (RSSI) of wireless signals as location fingerprint. Inspired by deep learning techniques for signal processing, we propose a deep neural network-based framework (DeepLoc) to implement Wi-Fi fingerprint positioning. In order to improve localization performance, we further design a network division based optimization algorithm. We first adopt greedy algorithm to locate the user in a sub-area, and then reconstruct a smaller fingerprint database, which is fed into the training model. Finally, we evaluate the proposed framework. Experimental results show that DeepLoc can improve the localization accuracy efficiently and obtain better performance.},
    author     = {Liu, Saining and Ren, Qianqian and Li, Jinbao and Xu, Hui},
    booktitle  = {2021 {{IEEE}} 23rd {{Int Conf}} on {{High Performance Computing}} {\&} {{Communications}}; 7th {{Int Conf}} on {{Data Science}} {\&} {{Systems}}; 19th {{Int Conf}} on {{Smart City}}; 7th {{Int Conf}} on {{Dependability}} in {{Sensor}}, {{Cloud}} {\&} {{Big Data Systems}} {\&} {{Application}} ({{HPCC}}/{{DSS}}/{{SmartCity}}/{{DependSys}})},
    doi        = {10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00255},
    keywords   = {deep learning,Deep learning,Fingerprint recognition,greedy algorithm,Greedy algorithms,Location awareness,positioning,Received signal strength indicator,Signal processing algorithms,sub-area,Training},
    month      = dec,
    pages      = {1735--1740},
    shorttitle = {{{DeepLoc}}},
    title      = {{{DeepLoc}}: {{A Deep Neural Network-based Indoor Positioning Framework}}},
    year       = {2021},
}


@article{Zhang202211918,
    author    = {Zhang, Chenhan and Cui, Lei and Yu, Shui and Yu, James J. Q.},
    copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
    doi       = {10.1109/JIOT.2021.3132363},
    issn      = {2327-4662, 2372-2541},
    journal   = {IEEE Internet of Things Journal},
    month     = jul,
    number    = {14},
    pages     = {11918--11931},
    title     = {A {{Communication-Efficient Federated Learning Scheme}} for {{IoT-Based Traffic Forecasting}}},
    volume    = {9},
    year      = {2022},
}


@article{Li2021,
    author  = {Li, Yuanlong and Zhou, Yiming and Zhu, Jun},
    doi     = {10.1109/TKDE.2020.2964658},
    journal = {IEEE Transactions on Knowledge and Data Engineering},
    number  = {8},
    pages   = {2967--2980},
    title   = {Adaptive Parameter Control for Non-Stationary Distributions},
    volume  = {33},
    year    = {2021},
}
@article{Kanchanamala2023,
    author            = {Kanchanamala, Pendela and Karnati, Ramesh and Bhaskar Reddy, Palagiri Vijaya},
    doi               = {10.1002/cpe.7618},
    journal           = {Concurrency and Computation: Practice and Experience},
    note              = {Cited by: 2},
    number            = {8},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {Hybrid optimization enabled deep learning and spark architecture using big data analytics for stock market forecasting},
    type              = {Article},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147335171&doi=10.1002%2fcpe.7618&partnerID=40&md5=8f3bea839e3eda361651f8453fe2d90c},
    volume            = {35},
    year              = {2023},
}


@article{Sun2022,
    author  = {Sun, Yuxin and Wang, Jian and Zhang, Haibo},
    doi     = {10.1109/TIFS.2022.3153212},
    journal = {IEEE Transactions on Information Forensics and Security},
    pages   = {1455--1467},
    title   = {Deep Learning for Phishing Detection},
    volume  = {17},
    year    = {2022},
}


@article{Chatterjee2021969,
    author    = {Chatterjee, Ankita and Saha, Jayasree and Mukherjee, Jayanta and Aikat, Subhas and Misra, Arundhati},
    copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
    doi       = {10.1109/LGRS.2020.2993095},
    issn      = {1545-598X, 1558-0571},
    journal   = {IEEE Geoscience and Remote Sensing Letters},
    month     = jun,
    number    = {6},
    pages     = {969--973},
    title     = {Unsupervised {{Land Cover Classification}} of {{Hybrid}} and {{Dual-Polarized Images Using Deep Convolutional Neural Network}}},
    volume    = {18},
    year      = {2021},
}


@article{Huang2023,
    author  = {Huang, Jiawei and Zhang, Min and Wang, Lei},
    doi     = {10.1109/TNNLS.2022.3178293},
    journal = {IEEE Transactions on Neural Networks and Learning Systems},
    number  = {8},
    pages   = {4259--4271},
    title   = {Dynamic Sparse Attention Mechanism for Memory Reduction},
    volume  = {34},
    year    = {2023},
}


@article{Javaid2022,
    author  = {Javaid, Muhammad and Haleem, Abid and Singh, Ravi Pratap},
    doi     = {10.1016/j.iot.2022.100516},
    journal = {Internet of Things},
    pages   = {100516},
    title   = {Digital Twin Technologies with Optimized Deep Learning},
    volume  = {19},
    year    = {2022},
}


@article{Zhou2021,
    author            = {Zhou, Zhou and Li, Fangmin and Yang, Shuiqiao},
    doi               = {10.1145/3462761},
    journal           = {ACM Transactions on Asian and Low-Resource Language Information Processing},
    note              = {Cited by: 9},
    number            = {5},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {A Novel Resource Optimization Algorithm Based on Clustering and Improved Differential Evolution Strategy under a Cloud Environment},
    type              = {Article},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120324865&doi=10.1145%2f3462761&partnerID=40&md5=05bfaa2ddde1c6e60614d2314f166132},
    volume            = {20},
    year              = {2021},
}


@inproceedings{Manoranjithem2023212,
    address   = {Bengaluru, India},
    author    = {{Manoranjithem} and Dhanasekaran, S. and Asokan, Anju and Kumar, Arvind and Yamini, C. and Tiwari, Mohit},
    booktitle = {2023 {{International Conference}} on {{Intelligent Data Communication Technologies}} and {{Internet}} of {{Things}} ({{IDCIoT}})},
    copyright = {https://doi.org/10.15223/policy-029},
    doi       = {10.1109/IDCIoT56793.2023.10053504},
    isbn      = {978-1-6654-7451-1},
    month     = jan,
    pages     = {212--216},
    publisher = {IEEE},
    title     = {An {{Intrusion Detection Approach}} Using {{Hierarchical Deep Learning-based Butterfly Optimization Algorithm}} in {{Big Data Platform}}},
    year      = {2023},
}


@article{Xiao2022,
    author  = {Xiao, Jianfeng and Wang, Li and Zhang, Han},
    doi     = {10.1109/TPAMI.2021.3078562},
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    number  = {9},
    pages   = {5127--5142},
    title   = {Gaussian Process Surrogate Models for Neural Architecture Search},
    volume  = {44},
    year    = {2022},
}


@article{Wu2022,
    author  = {Wu, Jianguo and Chen, Xin and Wang, Shuo},
    doi     = {10.1109/TKDE.2020.3038701},
    journal = {IEEE Transactions on Knowledge and Data Engineering},
    number  = {7},
    pages   = {3338--3351},
    title   = {Deep Learning for Financial Fraud Detection},
    volume  = {34},
    year    = {2022},
}


@article{Zhang20229876,
    author  = {Zhang, Wei and Lin, Xiaofeng and Chen, Jiayi},
    doi     = {10.1109/JIOT.2021.3135426},
    journal = {IEEE Internet of Things Journal},
    number  = {12},
    pages   = {9876--9889},
    title   = {Privacy-Preserving Federated Learning for IoT Edge Intelligence},
    volume  = {9},
    year    = {2022},
}


@article{Hassib20205573,
    author     = {Hassib, {\relax Eslam}. M. and {El-Desouky}, {\relax Ali}. I. and Labib, {\relax Labib}. M. and {El-kenawy}, El-Sayed M.},
    doi        = {10.1007/s00500-019-03901-y},
    issn       = {1432-7643, 1433-7479},
    journal    = {Soft Computing},
    langid     = {english},
    month      = apr,
    number     = {8},
    pages      = {5573--5592},
    shorttitle = {{{WOA}} + {{BRNN}}},
    title      = {{{WOA}} + {{BRNN}}: {{An}} Imbalanced Big Data Classification Framework Using {{Whale}} Optimization and Deep Neural Network},
    volume     = {24},
    year       = {2020},
}


@article{Lee2023,
    author  = {Lee, Jaehyun and Kwon, Hyunsung and Jung, Woojin},
    doi     = {10.1109/TPAMI.2022.3192712},
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    number  = {7},
    pages   = {8421--8436},
    title   = {Attention Mechanism Optimization for Transformer Inference},
    volume  = {45},
    year    = {2023},
}


@article{Zhang2021,
    author  = {Zhang, Yi and Li, Jianwei and Wang, Xin},
    doi     = {10.1016/j.patcog.2020.107809},
    journal = {Pattern Recognition},
    pages   = {107809},
    title   = {Dynamic Learning Rate Approach for Convolutional Neural Networks},
    volume  = {112},
    year    = {2021},
}


@article{Li2023,
    author  = {Li, Xiaohua and Chen, Wei and Zhang, Hongqing},
    doi     = {10.1093/bioinformatics/btad153},
    journal = {Bioinformatics},
    number  = {4},
    pages   = {btad153},
    title   = {Drug Discovery Optimization Using Computational Methods},
    volume  = {39},
    year    = {2023},
}


@article{Thoppil2021,
    abstractnote = {Abstract              An effective maintenance strategy to cut back maintenance costs and production loss with assured product quality has always been a major concern for industries. The Industry 4.0 era has built a wide acceptance for the predictive maintenance techniques in the remaining useful life (RUL) estimation of critical industrial systems. In this paper, long short-term memory (LSTM) and bidirectional-LSTM (bi-LSTM) deep neural architecture-based predictive algorithms are proposed for the RUL estimation of the lathe spindle unit. The deep learning algorithm is embedded within a Bayesian optimization algorithm for the self-optimization of its network structure and hyperparameters. The proposed deep learning algorithm is trained using lathe spindle health degradation data collected from an experimental accelerated run-to-failure test rig to evolve an RUL prediction model. The vibration signals representing lathe spindle health degradation from the health to faulty state are analyzed to extract time, frequency, and time-frequency domain features, which are then subjected to a neighborhood component analysis (NCA) based feature selection criteria. Finally, the selected relevant features are used to train the optimized LSTM/bi-LSTM network for RUL estimation. A comparison of the prediction results for Bayesian optimized LSTM/bi-LSTM network architectures and other prominent data-driven approaches are performed. The Bayesian optimized LSTM + bi-LSTM deep network architecture is observed to have the highest prediction accuracy for lathe spindle RUL estimation.},
    author       = {Thoppil, Nikhil M. and Vasu, V. and Rao, C. S. P.},
    doi          = {10.1115/1.4052838},
    issn         = {1530-9827, 1944-7078},
    journal      = {Journal of Computing and Information Science in Engineering},
    language     = {en},
    month        = apr,
    number       = {2},
    pages        = {021012},
    title        = {Bayesian Optimization LSTM/bi-LSTM Network With Self-Optimized Structure and Hyperparameters for Remaining Useful Life Estimation of Lathe Spindle Unit},
    url          = {https://asmedigitalcollection.asme.org/computingengineering/article/22/2/021012/1122887/Bayesian-Optimization-LSTM-bi-LSTM-Network-With},
    volume       = {22},
    year         = {2022},
}


@article{Liang2022,
    author  = {Liang, Xin and Wu, Zhenyu and Chen, Tianjian},
    doi     = {10.1109/MM.2022.3179084},
    journal = {IEEE Micro},
    number  = {5},
    pages   = {38--46},
    title   = {Specialized Operators for Low-Power Machine Learning Accelerators},
    volume  = {42},
    year    = {2022},
}


@article{Kanchanamala20232414IJACS,
    author  = {Kanchanamala, Padmavathi and Reddy, Chandra Sekhar and Kumar, Rajesh},
    doi     = {10.1109/ACCESS.2023.3235691},
    journal = {IEEE Access},
    pages   = {2414--2428},
    title   = {Exponential Chimp Optimization Algorithm for Fake News Detection},
    volume  = {11},
    year    = {2023},
}


@article{Liu2020257,
    author     = {Liu, Xiao and Zhang, Bin and Susarlia, Anjana and Padman, Rema},
    doi        = {10.25300/MISQ/2020/15107},
    issn       = {02767783, 21629730},
    journal    = {MIS Quarterly},
    month      = jan,
    number     = {1},
    pages      = {257--283},
    shorttitle = {Go to {{You Tube}} and {{Call Me}} in the {{Morning}}},
    title      = {Go to {{You Tube}} and {{Call Me}} in the {{Morning}}: {{Use}} of {{Social Media}} for {{Chronic Conditions}}},
    volume     = {44},
    year       = {2020},
}


@article{Wang2022939,
    author    = {Wang, Haozhao and Qu, Zhihao and Zhou, Qihua and Zhang, Haobo and Luo, Boyuan and Xu, Wenchao and Guo, Song and Li, Ruixuan},
    copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
    doi       = {10.1109/JIOT.2021.3111624},
    issn      = {2327-4662, 2372-2541},
    journal   = {IEEE Internet of Things Journal},
    month     = jan,
    number    = {2},
    pages     = {939--963},
    title     = {A {{Comprehensive Survey}} on {{Training Acceleration}} for {{Large Machine Learning Models}} in {{IoT}}},
    volume    = {9},
    year      = {2022},
}


@article{Jiang2022,
    author            = {Jiang, Jinsheng and Ren, Haoran and Zhang, Meng},
    doi               = {10.1109/LGRS.2021.3073560},
    journal           = {IEEE Geoscience and Remote Sensing Letters},
    note              = {Cited by: 50},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {A Convolutional Autoencoder Method for Simultaneous Seismic Data Reconstruction and Denoising},
    type              = {Article},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105059437&doi=10.1109%2fLGRS.2021.3073560&partnerID=40&md5=fd24b5d707947f637fdf8399ebfe1e7b},
    volume            = {19},
    year              = {2022},
}


@article{Zhang2023,
    author  = {Zhang, Wenqing and Liu, Jian and Wang, Zhen},
    doi     = {10.1109/TNNLS.2022.3224567},
    journal = {IEEE Transactions on Neural Networks and Learning Systems},
    number  = {12},
    pages   = {10283--10297},
    title   = {Distributed Reinforcement Learning Framework for Multiple Compute Clusters},
    volume  = {34},
    year    = {2023},
}


@article{Folino2022729,
    abstract = {Abstract             Predicting the final outcome of an ongoing process instance is a key problem in many real-life contexts. This problem has been addressed mainly by discovering a prediction model by using traditional machine learning methods and, more recently, deep learning methods, exploiting the supervision coming from outcome-class labels associated with historical log traces. However, a supervised learning strategy is unsuitable for important application scenarios where the outcome labels are known only for a small fraction of log traces. In order to address these challenging scenarios, a semi-supervised learning approach is proposed here, which leverages a multi-target DNN model supporting both outcome prediction and the additional auxiliary task of next-activity prediction. The latter task helps the DNN model avoid spurious trace embeddings and overfitting behaviors. In extensive experimentation, this approach is shown to outperform both fully-supervised and semi-supervised discovery methods using similar DNN architectures across different real-life datasets and label-scarce settings.},
    author   = {Folino, Francesco and Folino, Gianluigi and Guarascio, Massimo and Pontieri, Luigi},
    doi      = {10.1007/s12599-022-00749-9},
    issn     = {2363-7005, 1867-0202},
    journal  = {Business {\&} Information Systems Engineering},
    langid   = {english},
    month    = dec,
    number   = {6},
    pages    = {729--749},
    title    = {Semi-{{Supervised Discovery}} of {{DNN-Based Outcome Predictors}} from {{Scarcely-Labeled Process Logs}}},
    volume   = {64},
    year     = {2022},
}


@article{Garcia2021,
    author  = {Garcia, Rafael and Perez, Elena and Rodriguez, Maria},
    doi     = {10.1016/j.cviu.2020.103103},
    journal = {Computer Vision and Image Understanding},
    pages   = {103103},
    title   = {Model Quantization Approach for Computer Vision Tasks},
    volume  = {202},
    year    = {2021},
}


@article{MadhukarRao202127471,
    author            = {Madhukar Rao, G. and Dharavath, Ramesh},
    doi               = {10.1007/s11042-021-11059-9},
    journal           = {Multimedia Tools and Applications},
    note              = {Cited by: 6},
    number            = {18},
    pages             = {241--250},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {DSSAE-BBOA: deep learning-based weather big data analysis and visualization},
    type              = {Article},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106292045&doi=10.1007%2fs11042-021-11059-9&partnerID=40&md5=f876bb4243f6b1b296f326d7ca095897},
    volume            = {80},
    year              = {2021},
}


@article{Huo2020199573,
    author    = {Huo, Yusen and Tao, Qinghua and Hu, Jianming},
    copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
    doi       = {10.1109/ACCESS.2020.3034419},
    issn      = {2169-3536},
    journal   = {IEEE Access},
    pages     = {199573--199585},
    title     = {Cooperative {{Control}} for {{Multi-Intersection Traffic Signal Based}} on {{Deep Reinforcement Learning}} and {{Imitation Learning}}},
    volume    = {8},
    year      = {2020},
}


@article{Chen2021,
    author  = {Chen, Yiyi and Li, Wei and Wang, Xing},
    doi     = {10.1109/TFDS.2021.3094579},
    journal = {IEEE Transactions on Financial Data Science},
    number  = {3},
    pages   = {161--173},
    title   = {Deep Learning for High-Frequency Trading},
    volume  = {3},
    year    = {2021},
}


@article{Malik2023,
    author  = {Malik, Zeeshan and Khan, Muhammad and Ahmad, Farooq},
    doi     = {10.1109/TDSC.2022.3164484},
    journal = {IEEE Transactions on Dependable and Secure Computing},
    number  = {2},
    pages   = {1061--1074},
    title   = {Adaptive Learning Approaches for Zero-Day Attack Detection},
    volume  = {20},
    year    = {2023},
}


@article{Rawat2021,
    author  = {Rawat, Divya and Singh, Rahul and Agarwal, Amit},
    doi     = {10.4258/hir.2021.27.1.39},
    journal = {Healthcare Informatics Research},
    number  = {1},
    pages   = {39--58},
    title   = {Clinical Decision Support Systems Using Deep Learning},
    volume  = {27},
    year    = {2021},
}


@article{Khan2023,
    author  = {Khan, Aftab and Ahmed, Naveed and Malik, Adeel},
    doi     = {10.1109/TNNLS.2022.3199999},
    journal = {IEEE Transactions on Neural Networks and Learning Systems},
    number  = {10},
    pages   = {7083--7095},
    title   = {Event-Based Computing Approach for Sparsely Activated Neural Networks},
    volume  = {34},
    year    = {2023},
}


@article{Mehdiyev2020143,
    author  = {Mehdiyev, Nijat and Evermann, Joerg and Fettke, Peter},
    doi     = {10.1007/s12599-018-0551-3},
    issn    = {2363-7005, 1867-0202},
    journal = {Business {\&} Information Systems Engineering},
    langid  = {english},
    month   = apr,
    number  = {2},
    pages   = {143--157},
    title   = {A {{Novel Business Process Prediction Model Using}} a {{Deep Learning Method}}},
    volume  = {62},
    year    = {2020},
}


@article{Chen2023,
    author  = {Chen, Qiang and Zhang, Rui and Wang, Jie},
    doi     = {10.1109/TPDS.2022.3222222},
    journal = {IEEE Transactions on Parallel and Distributed Systems},
    number  = {5},
    pages   = {1489--1500},
    title   = {Mixed-Precision Training with Adaptive Batch Sizing for Deep Learning},
    volume  = {34},
    year    = {2023},
}


@article{Wang2021,
    author  = {Wang, Shuiqiang and Zhang, Li and Chen, Xiaoming},
    doi     = {10.1109/TNNLS.2020.3004080},
    journal = {IEEE Transactions on Neural Networks and Learning Systems},
    number  = {7},
    pages   = {3025--3039},
    title   = {Enhanced Adam Optimizer for Deep Neural Networks},
    volume  = {32},
    year    = {2021},
}


@article{Zhou20211,
    abstract = {Resource optimization algorithm based on clustering and improved differential evolution strategy, as a new global optimized algorithm, has wide applications in language translation, language processing, document understanding, cloud computing, and edge computing due to high efficiency. With the development of deep learning technology and the rise of big data, the resource optimization algorithm encounters a series of challenges, such as the workload imbalance and low resource utilization. To address the preceding problems, this study proposes a novel resource optimization algorithm based on clustering and an improved differential evolution strategy (Multi-objective Task Scheduling Strategy (MTSS)). Three indexes, namely task completion time, execution cost, and workload, of virtual machines are selected and used to build the fitness function of the MTSS algorithm. At the same time, the preprocessing state is set up to cluster according to the resource and task characteristics to reduce the magnitude of their matching scale. Moreover, to solve the workload imbalance among different resource sets, local resource tasks are reallocated using the Q-value method in the MTSS strategy to achieve workload balance of global resources and improve the resource utilization rate. Experiments are carried out to evaluate the effectiveness of the proposed algorithm. Results show that the proposed algorithm outperforms other algorithms in terms of task completion time, execution cost, and workload balancing.},
    author   = {Zhou, Zhou and Li, Fangmin and Yang, Shuiqiao},
    doi      = {10.1145/3462761},
    issn     = {2375-4699, 2375-4702},
    journal  = {ACM Transactions on Asian and Low-Resource Language Information Processing},
    langid   = {english},
    month    = sep,
    number   = {5},
    pages    = {1--15},
    title    = {A {{Novel Resource Optimization Algorithm Based}} on {{Clustering}} and {{Improved Differential Evolution Strategy Under}} a {{Cloud Environment}}},
    volume   = {20},
    year     = {2021},
}


@article{Rodriguez2022,
    author  = {Rodriguez, Juan and Martinez, Carlos and Sanchez, Rosa},
    doi     = {10.1007/s11069-022-05278-y},
    journal = {Natural Hazards},
    pages   = {1475--1497},
    title   = {Deep Learning Models for Earthquake Prediction},
    volume  = {112},
    year    = {2022},
}


@article{Kumar2022,
    author  = {Kumar, Rajesh and Singh, Sumit and Agarwal, Neha},
    doi     = {10.1109/TSC.2021.3064891},
    journal = {IEEE Transactions on Services Computing},
    number  = {5},
    pages   = {2718--2731},
    title   = {Particle Swarm Optimization with Gradient Descent for Federated Learning},
    volume  = {15},
    year    = {2022},
}


@conference{Wei20211370,
    author            = {Wei, Jia and Zhang, Xingjun and Ji, Zeyu and Li, Jingbo and Wei, Zheng},
    doi               = {10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00187},
    journal           = {19th IEEE International Symposium on Parallel and Distributed Processing with Applications, 11th IEEE International Conference on Big Data and Cloud Computing, 14th IEEE International Conference on Social Computing and Networking and 11th IEEE International Conference on Sustainable Computing and Communications, ISPA/BDCloud/SocialCom/SustainCom 2021},
    note              = {Cited by: 0},
    pages             = {241--250},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {PANDA: Population Automatic Neural Distributed Algorithm for Deep Leaning},
    type              = {Conference paper},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124134024&doi=10.1109%2fISPA-BDCloud-SocialCom-SustainCom52081.2021.00187&partnerID=40&md5=9a6daf8e5e4c8f199b58df2c4c13318b},
    year              = {2021},
}


@article{Wang2023,
    author  = {Wang, Xiaofei and Zhang, Ye and Han, Song},
    doi     = {10.1109/TMC.2022.3178888},
    journal = {IEEE Transactions on Mobile Computing},
    number  = {11},
    pages   = {6739--6753},
    title   = {Scaling Efficiency for Federated Learning Across Heterogeneous Edge Devices},
    volume  = {22},
    year    = {2023},
}


@article{Samuel202068,
    abstract  = {Over the last decades, load forecasting is used by power companies to balance energy demand and supply. Among the several load forecasting methods, medium-term load forecasting is necessary for grids maintenance planning, settings of electricity prices, and harmonizing energy sharing arrangement. The forecasting of the month ahead electrical loads provides the information required for the interchange of energy among power companies. For accurate load forecasting, this paper proposes a model for medium-term load forecasting that uses hourly electrical load and temperature data to predict month ahead hourly electrical loads. For data preprocessing, modified entropy mutual information-based feature selection is used. It eliminates the redundancy and irrelevancy of features from the data. We employ the conditional restricted Boltzmann machine (CRBM) for the load forecasting. A meta-heuristic optimization algorithm Jaya is used to improve the CRBMs accuracy rate and convergence. In addition, the consumers dynamic consumption behaviors are also investigated using a discrete-time Markov chain and an adaptive k-means is used to group their behaviors into clusters. We evaluated the proposed model using GEFCom2012 US utility dataset. Simulation results confirm that the proposed model achieves better accuracy, fast convergence, and low execution time as compared to other existing models in the literature.},
    author    = {Samuel, Omaji and Alzahrani, Fahad A. and Hussen Khan, Raja Jalees Ul and Farooq, Hassan and Shafiq, Muhammad and Afzal, Muhammad Khalil and Javaid, Nadeem},
    copyright = {https://creativecommons.org/licenses/by/4.0/},
    doi       = {10.3390/e22010068},
    issn      = {1099-4300},
    journal   = {Entropy},
    langid    = {english},
    month     = jan,
    number    = {1},
    pages     = {68},
    title     = {Towards {{Modified Entropy Mutual Information Feature Selection}} to {{Forecast Medium-Term Load Using}} a {{Deep Learning Model}} in {{Smart Homes}}},
    volume    = {22},
    year      = {2020},
}


@article{Hamza20226579,
    author            = {Hamza, Manar Ahmed and Abdalla Hashim, Aisha Hassan and Mohamed, Heba G. and Alotaibi, Saud S. and Mahgoub, Hany and Mehanna, Amal S. and Motwakel, Abdelwahed},
    doi               = {10.32604/cmc.2022.031303},
    journal           = {Computers, Materials and Continua},
    note              = {Cited by: 3; All Open Access, Gold Open Access},
    number            = {3},
    pages             = {241--250},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {Hyperparameter Tuned Deep Learning Enabled Intrusion Detection on Internet of Everything Environment},
    type              = {Article},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135032198&doi=10.32604%2fcmc.2022.031303&partnerID=40&md5=648796928ad8acd9afa9a697e93ff7c6},
    volume            = {73},
    year              = {2022},
}


@article{Zeng2024806,
    author  = {Zeng, Lei and Liu, Qi and Shen, Shigen and Liu, Xiaodong},
    doi     = {10.26599/TST.2023.9010058},
    issn    = {1007-0214},
    journal = {Tsinghua Science and Technology},
    month   = jun,
    number  = {3},
    pages   = {806--817},
    title   = {Improved {{Double Deep Q Network-Based Task Scheduling Algorithm}} in {{Edge Computing}} for {{Makespan Optimization}}},
    volume  = {29},
    year    = {2024},
}


@article{Torres202210533,
    author            = {Torres, J.F. and Martínez-Álvarez, F. and Troncoso, A.},
    doi               = {10.1007/s00521-021-06773-2},
    journal           = {Neural Computing and Applications},
    note              = {Cited by: 84; All Open Access, Green Open Access, Hybrid Gold Open Access},
    number            = {13},
    pages             = {241--250},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {A deep LSTM network for the Spanish electricity consumption forecasting},
    type              = {Article},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124270388&doi=10.1007%2fs00521-021-06773-2&partnerID=40&md5=f713a9db8549fc828c3a1dab440eb872},
    volume            = {34},
    year              = {2022},
}


@article{Brahmane202115253,
    author            = {Brahmane, Anilkumar V. and Krishna, B. Chaitanya},
    doi               = {10.1007/s00521-021-06145-w},
    journal           = {Neural Computing and Applications},
    note              = {Cited by: 7},
    number            = {22},
    pages             = {241--250},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {Big data classification using deep learning and apache spark architecture},
    type              = {Article},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109302985&doi=10.1007%2fs00521-021-06145-w&partnerID=40&md5=b1ef74db65c73b182a377bb9ffdebce2},
    volume            = {33},
    year              = {2021},
}


@article{Sheeba20231415,
    author            = {Sheeba, R. and Sharmila, R. and Alkhayyat, Ahmed and Malik, Rami Q.},
    doi               = {10.32604/csse.2023.034321},
    journal           = {Computer Systems Science and Engineering},
    note              = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
    number            = {2},
    pages             = {241--250},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {Modified Buffalo Optimization with Big Data Analytics Assisted Intrusion Detection Model},
    type              = {Article},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148225722&doi=10.32604%2fcsse.2023.034321&partnerID=40&md5=83a9c5911c65dfa39596c43d5c5a1e9a},
    volume            = {46},
    year              = {2023},
}


@article{Rajagopal20247175,
    abstract  = {Deep learning solutions in big data applications can benefit cloud centres and can also lead to network communication overhead. Typically, data collected from traffic are sent to the traffic management centre for analysis. However, this process can worsen the network route to the traffic management centre. A two-tier mechanism has been developed to address this issue, which performs vehicle speed estimation and traffic congestion detection for efficient traffic management. The real-time traffic video data are captured and the video frames are initially processed through a foreground extraction process, which extracts the temporarily stopped vehicles on the road by removing background pixels from the frames. The video frames are then wrapped in an up-down view to remove the influence of the observation angle. The traffic congestion is then detected accurately based on the traffic characteristics using the proposed Ensemble Random Forest-based Gradient Optimization (ERF-GO) algorithm. The generalization error occurs when learning complex features on frames is minimized using a gradient-based optimization (GO) algorithm. Finally, the learned information on traffic conditions is forwarded to the cloud and edge computing environments based on network connection speed. The efficiency of the proposed ERF-GO is investigated in terms of performance metrics, namely root mean square error, speed detection error, execution time, computational cost, accuracy, latency, workload balance, precision, recall, f-measure, and congestion detection error rate. The analytic result displays that the proposed ERF-GO algorithm attains a greater accuracy rate of about 98.65\% in detecting traffic congestion which is comparably higher than state-of-the-art methods.},
    author    = {Rajagopal, S. and Uma Devi, M. and Maria Jones, G. and Gomathy Nayagam, M.},
    doi       = {10.1080/03772063.2024.2350927},
    issn      = {0377-2063},
    journal   = {IETE Journal of Research},
    keywords  = {Edge computing,Ensemble random forest,Gradient-based optimization algorithm,Traffic congestion,Vehicle speed,Video frames},
    month     = sep,
    number    = {9},
    pages     = {7175--7191},
    publisher = {Taylor \& Francis},
    title     = {Ensemble {{Random Forest-based Gradient Optimization}} Based {{Energy Efficient Video Processing System}} for {{Smart Traffic Surveillance System}}},
    volume    = {70},
    year      = {2024},
}


@article{rumelhart1986learning,
    author  = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
    journal = {Nature},
    number  = {6088},
    pages   = {533--536},
    title   = {Learning representations by back-propagating errors},
    volume  = {323},
    year    = {1986},
}


@article{Yang202363,
    abstract   = {Unstructured multimedia data (text and audio) provides unprecedented opportunities to derive actionable decision-making in the financial industry, in areas such as portfolio and risk management. However, due to formidable methodological challenges, the promise of business value from unstructured multimedia data has not materialized. In this study, we use a design science approach to develop DeepVoice, a novel nonverbal predictive analysis system for financial risk prediction, in the setting of quarterly earnings conference calls. DeepVoice forecasts financial risk by leveraging not only what managers say (verbal linguistic cues) but also how managers say it (vocal cues) during the earnings conference calls. The design of DeepVoice addresses several challenges associated with the analysis of nonverbal communication. We also propose a two-stage deep learning model to effectively integrate managers sequential vocal and verbal cues. Using a unique dataset of 6,047 earnings call samples (audio recordings and textual transcripts) of S\&P 500 firms across four years, we show that DeepVoice yields remarkably lower risk forecast errors than that achieved by previous efforts. The improvement can also translate into nontrivial economic gains in options trading. The theoretical and practical implications of analyzing vocal cues are discussed.},
    author     = {Yang, Yi and Qin, Yu and Fan, Yangyang and Zhang, Zhongju},
    doi        = {10.25300/MISQ/2022/17062},
    issn       = {02767783, 21629730},
    journal    = {MIS Quarterly},
    month      = mar,
    number     = {1},
    pages      = {63--96},
    shorttitle = {Unlocking the {{Power}} of {{Voice}} for {{Financial Risk Prediction}}},
    title      = {Unlocking the {{Power}} of {{Voice}} for {{Financial Risk Prediction}}: {{A Theory-Driven Deep Learning Design Approach}}},
    volume     = {47},
    year       = {2023},
}


@article{KoumetioTekouabou20231421,
    author     = {Koumetio Tekouabou, St{\e}phane C{\e}dric and Diop, El Bachir and Azmi, Rida and Chenal, J{\e}r{\^o}me},
    doi        = {10.1007/s11831-022-09844-2},
    issn       = {1134-3060, 1886-1784},
    journal    = {Archives of Computational Methods in Engineering},
    langid     = {english},
    month      = mar,
    number     = {2},
    pages      = {1421--1438},
    shorttitle = {Artificial {{Intelligence Based Methods}} for {{Smart}} and {{Sustainable Urban Planning}}},
    title      = {Artificial {{Intelligence Based Methods}} for {{Smart}} and {{Sustainable Urban Planning}}: {{A Systematic Survey}}},
    volume     = {30},
    year       = {2023},
}


@article{Baniata20241963,
    abstract  = {Educational institutions are increasingly focused on supporting students who may be facing academic challenges, aiming to enhance their educational outcomes through targeted interventions. Within this framework, leveraging advanced deep learning techniques to develop recommendation systems becomes essential. These systems are designed to identify students at risk of underperforming by analyzing patterns in their historical academic data, thereby facilitating personalized support strategies. This research introduces an innovative deep learning model tailored for pinpointing students in need of academic assistance. Utilizing a Gated Recurrent Neural Network (GRU) architecture, the model is rich with features such as a dense layer, max-pooling layer, and the ADAM optimization method used to optimize performance. The effectiveness of this model was tested using a comprehensive dataset containing 15,165 records of student assessments collected across several academic institutions. A comparative analysis with existing educational recommendation models, like Recurrent Neural Network (RNN), AdaBoost, and Artificial Immune Recognition System v2, highlights the superior accuracy of the proposed GRU model, which achieved an impressive overall accuracy of 99.70\%. This breakthrough underscores the models potential in aiding educational institutions to proactively support students, thereby mitigating the risks of underachievement and dropout.},
    author    = {Baniata, Laith H. and Kang, Sangwoo and Alsharaiah, Mohammad A. and Baniata, Mohammad H.},
    copyright = {https://creativecommons.org/licenses/by/4.0/},
    doi       = {10.3390/app14051963},
    issn      = {2076-3417},
    journal   = {Applied Sciences},
    langid    = {english},
    month     = feb,
    number    = {5},
    pages     = {1963},
    title     = {Advanced {{Deep Learning Model}} for {{Predicting}} the {{Academic Performances}} of {{Students}} in {{Educational Institutions}}},
    volume    = {14},
    year      = {2024},
}


@article{Lin2022,
    author  = {Lin, Yuxiang and Wang, Zhilin and Chen, Kai},
    journal = {Advances in Neural Information Processing Systems},
    pages   = {15789--15801},
    title   = {Gradient Checkpointing Approach for Large Language Models},
    volume  = {35},
    year    = {2022},
}


@article{Wang2021Online,
    author  = {Wang, Jialong and Xu, Shuai and Xu, Bo},
    doi     = {10.1109/TNNLS.2020.2978554},
    journal = {IEEE Transactions on Neural Networks and Learning Systems},
    number  = {2},
    pages   = {710--722},
    title   = {Multi-Armed Bandit Formulation for Online Hyperparameter Optimization},
    volume  = {32},
    year    = {2021},
}


@article{Park2021,
    author  = {Park, Sungjin and Kim, Jungwoo and Lee, Jaeho},
    doi     = {10.1016/j.jretconser.2021.102723},
    journal = {Journal of Retailing and Consumer Services},
    pages   = {102723},
    title   = {Customer Behavior Modeling using Deep Learning},
    volume  = {63},
    year    = {2021},
}


@article{Zhang20221,
    author  = {Zhang, Chengming and Yang, Yunxin and Chen, Wei},
    journal = {Proceedings of Machine Learning and Systems},
    pages   = {267--281},
    title   = {Tensor Parallelism for Large-Scale Model Training},
    volume  = {4},
    year    = {2022},
}


@article{Pustokhin2021,
    author            = {Pustokhin, Denis A. and Pustokhina, Irina V. and Rani, Poonam and Kansal, Vineet and Elhoseny, Mohamed and Joshi, Gyanendra Prasad and Shankar, K.},
    doi               = {10.1016/j.compeleceng.2021.107376},
    journal           = {Computers and Electrical Engineering},
    note              = {Cited by: 26},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {Optimal deep learning approaches and healthcare big data analytics for mobile networks toward 5G},
    type              = {Article},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112824101&doi=10.1016%2fj.compeleceng.2021.107376&partnerID=40&md5=10252347ec36ea5f094e4e610b5305b2},
    volume            = {95},
    year              = {2021},
}


@article{Babu20233621,
    author            = {Babu, Ierin and Mathusoothana, R. and Kumar, S.},
    doi               = {10.32604/iasc.2023.033791},
    journal           = {Intelligent Automation and Soft Computing},
    note              = {Cited by: 5; All Open Access, Hybrid Gold Open Access},
    number            = {3},
    pages             = {241--250},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {Evolutionary Algorithm Based Feature Subset Selection for Students Academic Performance Analysis},
    type              = {Article},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150772054&doi=10.32604%2fiasc.2023.033791&partnerID=40&md5=517db7668ec7947497b5959ac1936631},
    volume            = {36},
    year              = {2023},
}


@article{Eid20223845,
    abstract  = {Recent technologies such as artificial intelligence, machine learning, and big data are essential for supporting healthcare monitoring systems, particularly for monitoring Monkeypox confirmed cases. Infected and uninfected cases around the world have contributed to a growing dataset, which is publicly available and can be used by artificial intelligence and machine learning to predict the confirmed cases of Monkeypox at an early stage. Motivated by this, we propose in this paper a new approach for accurate prediction of the Monkeypox confirmed cases based on an optimized Long Short-Term Memory (LSTM) deep network. To fine-tune the hyper-parameters of the LSTM-based deep network, we employed the Al-Biruni Earth Radius (BER) optimization algorithm; thus, the proposed approach is denoted by BER-LSTM. Experimental results show the effectiveness of the proposed approach when assessed using various evaluation criteria, such as Mean Bias Error, which is recorded as (0.06) using BER-LSTM. To prove the superiority of the proposed approach, six different machine learning models are included in the conducted experiments. In addition, four different optimization algorithms are considered for comparison purposes. The results of this comparison confirmed the superiority of the proposed approach. On the other hand, several statistical tests are applied to analyze the stability and significance of the proposed approach. These tests include one-way Analysis of Variance (ANOVA), Wilcoxon, and regression tests. The results of these tests emphasize the robustness, significance, and efficiency of the proposed approach.},
    author    = {Eid, Marwa M. and {El-Kenawy}, El-Sayed M. and Khodadadi, Nima and Mirjalili, Seyedali and Khodadadi, Ehsaneh and Abotaleb, Mostafa and Alharbi, Amal H. and Abdelhamid, Abdelaziz A. and Ibrahim, Abdelhameed and Amer, Ghada M. and Kadi, Ammar and Khafaga, Doaa Sami},
    copyright = {https://creativecommons.org/licenses/by/4.0/},
    doi       = {10.3390/math10203845},
    issn      = {2227-7390},
    journal   = {Mathematics},
    langid    = {english},
    month     = oct,
    number    = {20},
    pages     = {3845},
    title     = {Meta-{{Heuristic Optimization}} of {{LSTM-Based Deep Network}} for {{Boosting}} the {{Prediction}} of {{Monkeypox Cases}}},
    volume    = {10},
    year      = {2022},
}


@article{Zhou2022764,
    author            = {Zhou, Yangfan and Huang, Kaizhu and Cheng, Cheng and Wang, Xuguang and Liu, Xin},
    doi               = {10.1007/s12559-021-09985-9},
    journal           = {Cognitive Computation},
    note              = {Cited by: 6},
    number            = {2},
    pages             = {241--250},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {LightAdam: Towards a Fast and Accurate Adaptive Momentum Online Algorithm},
    type              = {Article},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122690180&doi=10.1007%2fs12559-021-09985-9&partnerID=40&md5=c844748d8c9ef7dbe7b6a03b7618327d},
    volume            = {14},
    year              = {2022},
}


@article{Gupta2022,
    author  = {Gupta, Saurabh and Verma, Ajay and Singh, Devendra},
    doi     = {10.1109/JIOT.2021.3132845},
    journal = {IEEE Internet of Things Journal},
    number  = {10},
    pages   = {7865--7877},
    title   = {Botnet Identification in IoT Networks},
    volume  = {9},
    year    = {2022},
}


@article{Kratsch2021261,
    abstract   = {Predictive process monitoring aims at forecasting the behavior, performance, and outcomes of business processes at runtime. It helps identify problems before they occur and re-allocate resources before they are wasted. Although deep learning (DL) has yielded breakthroughs, most existing approaches build on classical machine learning (ML) techniques, particularly when it comes to outcome-oriented predictive process monitoring. This circumstance reflects a lack of understanding about which event log properties facilitate the use of DL techniques. To address this gap, the authors compared the performance of DL (i.e., simple feedforward deep neural networks and long short term memory networks) and ML techniques (i.e., random forests and support vector machines) based on five publicly available event logs. It could be observed that DL generally outperforms classical ML techniques. Moreover, three specific propositions could be inferred from further observations: First, the outperformance of DL techniques is particularly strong for logs with a high variant-to-instance ratio (i.e., many non-standard cases). Second, DL techniques perform more stably in case of imbalanced target variables, especially for logs with a high event-to-activity ratio (i.e., many loops in the control flow). Third, logs with a high activity-to-instance payload ratio (i.e., input data is predominantly generated at runtime) call for the application of long short term memory networks. Due to the purposive sampling of event logs and techniques, these findings also hold for logs outside this study.},
    author     = {Kratsch, Wolfgang and Manderscheid, Jonas and R{\"o}glinger, Maximilian and Seyfried, Johannes},
    doi        = {10.1007/s12599-020-00645-0},
    issn       = {1867-0202},
    journal    = {Business {\journal = {Business \& Information Systems Engineering}} Information Systems Engineering},
    keywords   = {Business process management,Deep learning,Machine learning,Outcome prediction,Predictive process monitoring},
    langid     = {english},
    month      = jun,
    number     = {3},
    pages      = {261--276},
    shorttitle = {Machine {{Learning}} in {{Business Process Monitoring}}},
    title      = {Machine {{Learning}} in {{Business Process Monitoring}}: {{A Comparison}} of {{Deep Learning}} and {{Classical Approaches Used}} for {{Outcome Prediction}}},
    volume     = {63},
    year       = {2021},
}


@article{Jin2022,
    author  = {Jin, Sungho and Kim, Hyungjun and Park, Jongwoo},
    doi     = {10.1109/TCAD.2021.3109494},
    journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
    number  = {9},
    pages   = {2890--2903},
    title   = {Specialized Optimization for Edge NPU Implementations},
    volume  = {41},
    year    = {2022},
}


@article{Mohyuddin2023100317,
    author  = {Mohyuddin, Hassan and Moosavi, Syed Kumayl Raza and Zafar, Muhammad Hamza and Sanfilippo, Filippo},
    doi     = {10.1016/j.array.2023.100317},
    issn    = {25900056},
    journal = {Array},
    langid  = {english},
    month   = sep,
    pages   = {100317},
    title   = {A Comprehensive Framework for Hand Gesture Recognition Using Hybrid-Metaheuristic Algorithms and Deep Learning Models},
    volume  = {19},
    year    = {2023},
}


@article{Nguyen2021,
    author  = {Nguyen, Thi and Tran, Minh and Duong, Thanh},
    doi     = {10.1016/j.cie.2020.107023},
    journal = {Computers {\&} Industrial Engineering},
    pages   = {107023},
    title   = {Deep Learning for Supply Chain Optimization},
    volume  = {152},
    year    = {2021},
}


@article{Sagu202535,
    author    = {Sagu, Amit and Gill, Nasib Singh and Gulia, Preeti and Priyadarshini, Ishaani and Chatterjee, Jyotir Moy},
    copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
    doi       = {10.1109/TBDATA.2024.3372368},
    issn      = {2332-7790, 2372-2096},
    journal   = {IEEE Transactions on Big Data},
    month     = feb,
    number    = {1},
    pages     = {35--46},
    title     = {Hybrid {{Optimization Algorithm}} for {{Detection}} of {{Security Attacks}} in {{IoT-Enabled Cyber-Physical Systems}}},
    volume    = {11},
    year      = {2025},
}


@article{Xu2023034501,
    abstract   = {Abstract             The increase of the spatial resolution in numerical computation always leads to a decrease in computing efficiency with respect to the constraint of mesh density. In response to this problem of the inability to perform numerical computation, we propose a novel method to boost the mesh-density in the finite element method (FEM) within 2D domains. Running on the von Mises stress fields of the 2D plane-strain problems computed by FEM, the proposed method utilizes a deep neural network named SMNet to learn a nonlinear mapping from low mesh-density to high mesh-density in stress fields and realizes the improvement of numerical computation accuracy and efficiency simultaneously. By introducing residual density blocks into SMNet, we can extract abundant local features and improve prediction capacity. The result indicates that SMNet can effectively increase the spatial resolution of stress fields under multiple scaling factors in mesh-density: 2 {\texttimes}, 3 {\texttimes}, and 4 {\texttimes}. Compared with the targets, the relative error of SMNet is 1.67\%, showing better performance than many other methods. SMNet can be generically used as an enhanced mesh-density boosting model of 2D physical fields for mesh-based numerical methods.},
    author     = {Xu, Handing and Nie, Zhenguo and Xu, Qingfeng and Li, Yaguan and Xie, Fugui and Liu, Xin-Jun},
    doi        = {10.1115/1.4054687},
    issn       = {1530-9827, 1944-7078},
    journal    = {Journal of Computing and Information Science in Engineering},
    langid     = {english},
    month      = jun,
    number     = {3},
    pages      = {034501},
    shorttitle = {{{SuperMeshing}}},
    title      = {{{SuperMeshing}}: {{Boosting}} the {{Mesh Density}} of {{Stress Field}} in {{Plane-Strain Problems Using Deep Learning Method}}},
    volume     = {23},
    year       = {2023},
}


@article{Moayedi20211331,
    abstract  = {A reliable prediction of sustainable energy consumption is key for designing environmentally friendly buildings. In this study, three novel hybrid intelligent methods, namely the grasshopper optimization algorithm (GOA), wind-driven optimization (WDO), and biogeography-based optimization (BBO), are employed to optimize the multitarget prediction of heating loads (HLs) and cooling loads (CLs) in the heating, ventilation and air conditioning (HVAC) systems. Concerning the optimization of the applied algorithms, a series of swarm-based iterations are performed, and the best structure is proposed for each model. The GOA, WDO, and BBO algorithms are mixed with a class of feedforward artificial neural networks (ANNs), which is called a multi-layer perceptron (MLP) to predict the HL and CL. According to the sensitivity analysis, the WDO with swarm size = 500 proposes the most-fitted ANN. The proposed WDO-ANN provided an accurate prediction in terms of heating load (training (R2 correlation = 0.977 and RMSE error = 0.183) and testing (R2 correlation = 0.973 and RMSE error = 0.190)) and yielded the best-fitted prediction in terms of cooling load (training (R2 correlation = 0.99 and RMSE error = 0.147) and testing (R2 correlation = 0.99 and RMSE error = 0.148)).},
    author    = {Moayedi, Hossein and Mosavi, Amir},
    copyright = {https://creativecommons.org/licenses/by/4.0/},
    doi       = {10.3390/en14051331},
    issn      = {1996-1073},
    journal   = {Energies},
    langid    = {english},
    month     = mar,
    number    = {5},
    pages     = {1331},
    title     = {Double-{{Target Based Neural Networks}} in {{Predicting Energy Consumption}} in {{Residential Buildings}}},
    volume    = {14},
    year      = {2021},
}


@article{AhmedHamza20226563,
    author  = {Ahmed Hamza, Manar and Alsolai, Hadeel and S. Alzahrani, Jaber and Alamgeer, Mohammad and Mahmoud Sayed, Mohamed and Sarwar Zamani, Abu and Yaseen, Ishfaq and Motwakel, Abdelwahed},
    doi     = {10.32604/cmc.2022.031541},
    issn    = {1546-2226},
    journal = {Computers, Materials {\journal = {Computers, Materials \& Continua}} Continua},
    langid  = {english},
    number  = {3},
    pages   = {6563--6577},
    title   = {Intelligent {{Slime Mould Optimization}} with {{Deep Learning Enabled Traffic Prediction}} in {{Smart Cities}}},
    volume  = {73},
    year    = {2022},
}


@article{Huang2020,
    author  = {Huang, Li and Chen, Jianhua and Yang, Xin},
    doi     = {10.1038/s41467-020-19673-1},
    journal = {Nature Communications},
    pages   = {5887},
    title   = {Deep Learning for Pandemic Forecasting},
    volume  = {11},
    year    = {2020},
}


@article{Nguyen2023,
    author  = {Nguyen, Thanh and Le, Minh and Pham, Huy},
    doi     = {10.1109/JIOT.2022.3201111},
    journal = {IEEE Internet of Things Journal},
    number  = {6},
    pages   = {5478--5490},
    title   = {Extreme Model Compression for Edge Deployment},
    volume  = {10},
    year    = {2023},
}


@article{Pandey2023,
    author            = {Pandey, Bishwajeet Kumar and M．R．M．, Veeramanickam and Ahmad, Shabeer and Rodriguez, Ciro and Esenarro, Doris},
    doi               = {10.1016/j.cose.2022.102975},
    journal           = {Computers and Security},
    note              = {Cited by: 21},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {ExpSSOA-Deep maxout: Exponential Shuffled shepherd optimization based Deep maxout network for intrusion detection using big data in cloud computing framework},
    type              = {Article},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142248862&doi=10.1016%2fj.cose.2022.102975&partnerID=40&md5=b57510e55fffcc36d221738f93ae5edb},
    volume            = {124},
    year              = {2023},
}


@article{Li2023Battery,
    author            = {Li, Chaoran and Han, Xianjie and Zhang, Qiang and Li, Menghan and Rao, Zhonghao and Liao, Wei and Liu, Xiaori and Liu, Xinjian and Li, Gang},
    doi               = {10.1016/j.est.2023.107184},
    journal           = {Journal of Energy Storage},
    note              = {Cited by: 2},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {State-of-health and remaining-useful-life estimations of lithium-ion battery based on temporal convolutional network-long short-term memory},
    type              = {Article},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147335171&doi=10.1002%2fcpe.7618&partnerID=40&md5=8f3bea839e3eda361651f8453fe2d90c},
    volume            = {65},
    year              = {2023},
}


@article{Palermo202240,
    author     = {Palermo, Marcelo Benedeti and Policarpo, Lucas Micol and Costa, Cristiano Andr{\e} Da and Righi, Rodrigo Da Rosa},
    doi        = {10.1007/s13721-022-00384-0},
    issn       = {2192-6662, 2192-6670},
    journal    = {Network Modeling Analysis in Health Informatics and Bioinformatics},
    langid     = {english},
    month      = dec,
    number     = {1},
    pages      = {40},
    shorttitle = {Tracking Machine Learning Models for Pandemic Scenarios},
    title      = {Tracking Machine Learning Models for Pandemic Scenarios: A Systematic Review of Machine Learning Models That Predict Local and Global Evolution of Pandemics},
    volume     = {11},
    year       = {2022},
}


@article{Li2020,
    author  = {Li, Zhihua and Wang, Ruosi and Yu, Dongmin},
    doi     = {10.1109/ACCESS.2020.3021736},
    journal = {IEEE Access},
    pages   = {163535--163556},
    title   = {Deep Learning for IoT Applications},
    volume  = {8},
    year    = {2020},
}


@article{Abayomi2021,
    author  = {Abayomi, John and Williams, Robert and Thomas, Andrew},
    doi     = {10.1109/TNSM.2021.3053410},
    journal = {IEEE Transactions on Network and Service Management},
    number  = {2},
    pages   = {1880--1893},
    title   = {Optimized CNN Architectures for Network Traffic Analysis},
    volume  = {18},
    year    = {2021},
}


@article{Tekouabou20241079,
    author     = {Tekouabou, St{\e}phane C. K. and Gherghina, {\c S}tefan Cristian and Kameni, Eric D{\e}sir{\e} and Filali, Youssef and Idrissi Gartoumi, Khalil},
    doi        = {10.1007/s11831-023-10010-5},
    issn       = {1134-3060, 1886-1784},
    journal    = {Archives of Computational Methods in Engineering},
    langid     = {english},
    month      = mar,
    number     = {2},
    pages      = {1079--1095},
    shorttitle = {{{AI-Based}} on {{Machine Learning Methods}} for {{Urban Real Estate Prediction}}},
    title      = {{{AI-Based}} on {{Machine Learning Methods}} for {{Urban Real Estate Prediction}}: {{A Systematic Survey}}},
    volume     = {31},
    year       = {2024},
}


@article{Xu2022,
    author  = {Xu, Tianfeng and Liang, Jian and Zhang, Xuesong},
    doi     = {10.1016/j.jocs.2022.101735},
    journal = {Journal of Computational Science},
    pages   = {101735},
    title   = {Distributed Optimization Approach for Scientific Machine Learning},
    volume  = {62},
    year    = {2022},
}


@article{Oberdorf202349,
    abstract = {Abstract             Ever-growing data availability combined with rapid progress in analytics has laid the foundation for the emergence of business process analytics. Organizations strive to leverage predictive process analytics to obtain insights. However, current implementations are designed to deal with homogeneous data. Consequently, there is limited practical use in an organization with heterogeneous data sources. The paper proposes a method for predictive end-to-end enterprise process network monitoring leveraging multi-headed deep neural networks to overcome this limitation. A case study performed with a medium-sized German manufacturing company highlights the methods utility for organizations.},
    author   = {Oberdorf, Felix and Schaschek, Myriam and Weinzierl, Sven and Stein, Nikolai and Matzner, Martin and Flath, Christoph M.},
    doi      = {10.1007/s12599-022-00778-4},
    issn     = {2363-7005, 1867-0202},
    journal  = {Business {\journal = {Business \& Information Systems Engineering}} Information Systems Engineering},
    langid   = {english},
    month    = feb,
    number   = {1},
    pages    = {49--64},
    title    = {Predictive {{End-to-End Enterprise Process Network Monitoring}}},
    volume   = {65},
    year     = {2023},
}


@article{Wu2021,
    author  = {Wu, Jianqiao and Li, Menghan and Zhang, Tong},
    doi     = {10.1162/tacl_a_00387},
    journal = {Transactions of the Association for Computational Linguistics},
    pages   = {629--645},
    title   = {Parameter Sharing Technique for Transformer Models},
    volume  = {9},
    year    = {2021},
}


@article{Li2022,
    author  = {Li, Jinbao and Wang, Xiang and Chen, Han},
    doi     = {10.1016/j.knosys.2021.107658},
    journal = {Knowledge-Based Systems},
    pages   = {107658},
    title   = {Transfer Learning with Hyperparameter Optimization for Deep Neural Networks},
    volume  = {235},
    year    = {2022},
}


@article{Rahman2023,
    author  = {Rahman, Abdur and Khan, Zafar and Javaid, Nadeem},
    doi     = {10.1109/MNET.2022.3191666},
    journal = {IEEE Network},
    number  = {1},
    pages   = {46--52},
    title   = {DDoS Attack Mitigation Through Optimized Deep Learning},
    volume  = {37},
    year    = {2023},
}
@article{Banchhor2022,
    author            = {Banchhor, Chitrakant and Srinivasu, N.},
    doi               = {10.4018/IJSIR.302612},
    journal           = {International Journal of Swarm Intelligence Research},
    note              = {Cited by: 1},
    number            = {1},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {Grey Wolf Shuffled Shepherd Optimization Algorithm-Based Hybrid Deep Learning Classifier for Big Data Classification},
    type              = {Article},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153934848&doi=10.4018%2fIJSIR.302612&partnerID=40&md5=267d8fd52cdefc41da6d3baa0499cdf2},
    volume            = {13},
    year              = {2022},
}

@article{ataei2024filtering,
    author    = {Ataei, Pouya and Regula, Sri and Staegemann, Daniel and Malgaonkar, Saurabh},
    journal   = {AI},
    number    = {4},
    pages     = {2237--2259},
    publisher = {MDPI},
    title     = {Filtering Useful App Reviews Using Na{\"\i}ve Bayes—Which Na{\"\i}ve Bayes?},
    volume    = {5},
    year      = {2024},
}


@article{Kim2022,
    author  = {Kim, Hyunjoon and Park, Joonho and Lee, Sunghyun},
    doi     = {10.1109/TCAD.2021.3089276},
    journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
    number  = {8},
    pages   = {2567--2580},
    title   = {Hardware-Aware Optimization Techniques for Inference Latency Reduction},
    volume  = {41},
    year    = {2022},
}


@article{Prasanth2019282,
    author  = {Prasanth, T. and Gunasekaran, M.},
    doi     = {10.1007/s11036-018-1204-y},
    issn    = {1383-469X, 1572-8153},
    journal = {Mobile Networks and Applications},
    langid  = {english},
    month   = feb,
    number  = {1},
    pages   = {282--294},
    title   = {Effective {{Big Data Retrieval Using Deep Learning Modified Neural Networks}}},
    volume  = {24},
    year    = {2019},
}


@article{Shan2020224884,
    author    = {Shan, Hongtao and Sun, Yuanyuan and Zhang, Wenjun and Kudreyko, Aleksey and Ren, Lijia},
    copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
    doi       = {10.1109/ACCESS.2020.3007776},
    issn      = {2169-3536},
    journal   = {IEEE Access},
    pages     = {224884--224894},
    title     = {Reliability {{Analysis}} of {{Power Distribution Network Based}} on {{PSO-DBN}}},
    volume    = {8},
    year      = {2020},
}


@article{Almutairi20225924,
    abstract  = {Recent studies have witnessed remarkable merits of metaheuristic algorithms in optimization problems. Due to the significance of the early analysis of the thermal load in energy-efficient buildings, this work introduces and compares four novel optimizer techniques---the firefly algorithm (FA), optics-inspired optimization (OIO), shuffled complex evolution (SCE), and teaching--learning-based optimization (TLBO)---for an accurate prediction of the heating load (HL). The models are applied to a multilayer perceptron (MLP) neural network to surmount its computational shortcomings. The models are fed by a literature-based dataset obtained for residential buildings. The results revealed that all models used are capable of properly analyzing and predicting the HL pattern. A comparison between them, however, showed that the TLBO-MLP with the coefficients of determination 0.9610 vs. 0.9438, 0.9373, and 0.9556 (respectively, for FA-MLP, OIO-MLP, and SCE-MLP) and the root mean square error of 2.1103 vs. 2.5456, 2.7099, and 2.2774 presents the most reliable approximation of the HL. It also surpassed several methods used in previous studies. Thus, the developed TLBO-MLP can be a beneficial model for subsequent practical applications.},
    author    = {Almutairi, Khalid and Algarni, Salem and Alqahtani, Talal and Moayedi, Hossein and Mosavi, Amir},
    copyright = {https://creativecommons.org/licenses/by/4.0/},
    doi       = {10.3390/su14105924},
    issn      = {2071-1050},
    journal   = {Sustainability},
    langid    = {english},
    month     = may,
    number    = {10},
    pages     = {5924},
    title     = {A {{TLBO-Tuned Neural Processor}} for {{Predicting Heating Load}} in {{Residential Buildings}}},
    volume    = {14},
    year      = {2022},
}


@article{Thapaliya202316,
    author            = {Thapaliya, Suman and Sharma, Pawan Kumar},
    doi               = {10.1007/s10776-022-00586-3},
    journal           = {International Journal of Wireless Information Networks},
    note              = {Cited by: 2},
    number            = {1},
    pages             = {241--250},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {Cyber Forensic Investigation in IoT Using Deep Learning Based Feature Fusion in Big Data},
    type              = {Article},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143391443&doi=10.1007%2fs10776-022-00586-3&partnerID=40&md5=183a61751cdbbfe9dc4479d48e7a4dc0},
    volume            = {30},
    year              = {2023},
}


@article{Wu2023,
    author  = {Wu, Xiangrong and Zhang, Li and Chen, Yiwen},
    doi     = {10.1016/j.neunet.2022.11.019},
    journal = {Neural Networks},
    pages   = {142--155},
    title   = {Adaptive Optimization Strategy for Non-Stationary Data},
    volume  = {158},
    year    = {2023},
}


@article{Ampel2024137,
    abstract   = {The rapid proliferation of complex information systems has been met by an ever-increasing quantity of exploits that can cause irreparable cyber breaches. To mitigate these cyber threats, academia and industry have placed a significant focus on proactively identifying and labeling exploits developed by the international hacker community. However, prevailing approaches for labeling exploits in hacker forums do not leverage metadata from exploit darknet markets or public exploit repositories to enhance labeling performance. In this study, we adopted the computational design science paradigm to develop a novel information technology artifact, the deep transfer learning exploit labeler (DTL-EL). DTL-EL incorporates a pre-initialization design, multi-layer deep transfer learning (DTL), and a self-attention mechanism to automatically label exploits in hacker forums. We rigorously evaluated the proposed DTL-EL against state-of-the-art non-DTL benchmark methods based in classical machine learning and deep learning. Results suggest that the proposed DTL-EL significantly outperforms benchmark methods based on accuracy, precision, recall, and F1-score. Our proposed DTL-EL framework provides important practical implications for key stakeholders such as cybersecurity managers, analysts, and educators.},
    author     = {Ampel, Benjamin and Samtani, Sagar and Zhu, Hongyi and Chen, Hsinchun},
    doi        = {10.25300/MISQ/2023/17316},
    issn       = {02767783, 21629730},
    journal    = {MIS Quarterly},
    month      = mar,
    number     = {1},
    pages      = {137--166},
    shorttitle = {Creating {{Proactive Cyber Threat Intelligence}} with {{Hacker Exploit Labels}}},
    title      = {Creating {{Proactive Cyber Threat Intelligence}} with {{Hacker Exploit Labels}}: {{A Deep Transfer Learning Approach}}},
    volume     = {48},
    year       = {2024},
}


@article{Zhang2022,
    author  = {Zhang, Hao and Li, Zeyu and Wang, Jianyu},
    doi     = {10.1109/TPDS.2021.3135689},
    journal = {IEEE Transactions on Parallel and Distributed Systems},
    number  = {8},
    pages   = {1878--1890},
    title   = {Improved Communication Protocol for Distributed Deep Learning},
    volume  = {33},
    year    = {2022},
}


@article{Khan2020,
    author  = {Khan, Rafiullah and Schmidt, Bernhard and Kurtz, William},
    doi     = {10.1109/TNNLS.2020.2978577},
    journal = {IEEE Transactions on Neural Networks and Learning Systems},
    number  = {11},
    pages   = {5051--5065},
    title   = {Bayesian Optimization for Hyperparameter Tuning in Resource-Constrained Environments},
    volume  = {31},
    year    = {2020},
}


@article{SulthanAlikhan2023,
    author            = {Sulthan Alikhan, J. and Alageswaran, R. and Miruna Joe Amali, S.},
    doi               = {10.1016/j.bspc.2023.105011},
    journal           = {Biomedical Signal Processing and Control},
    note              = {Cited by: 14},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {Self-attention convolutional neural network optimized with season optimization algorithm Espoused Chronic Kidney Diseases Diagnosis in Big Data System},
    type              = {Article},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160015120&doi=10.1016%2fj.bspc.2023.105011&partnerID=40&md5=31dfae96f4a9b24536e66f732392b98d},
    volume            = {85},
    year              = {2023},
}


@article{Vijayalakshmi2022,
    author  = {Vijayalakshmi, K. and Rajesh, S. and Kumar, P.},
    doi     = {10.1007/s10916-022-01782-7},
    journal = {Journal of Medical Systems},
    number  = {3},
    pages   = {18--31},
    title   = {Deep Learning for Patient Monitoring Systems},
    volume  = {46},
    year    = {2022},
}


@inproceedings{Wang2018175,
    address   = {Chengdu},
    author    = {Wang, Shuqiang and Shen, Yanyan and Zeng, Dewei and Hu, Yong},
    booktitle = {2018 {{International Conference}} on {{Artificial Intelligence}} and {{Big Data}} ({{ICAIBD}})},
    doi       = {10.1109/ICAIBD.2018.8396189},
    isbn      = {978-1-5386-6987-7},
    month     = may,
    pages     = {175--178},
    publisher = {IEEE},
    title     = {Bone Age Assessment Using Convolutional Neural Networks},
    year      = {2018},
}


@article{Zhu2022,
    author  = {Zhu, Xiaoning and Chen, Bo and Yang, Fan},
    doi     = {10.1016/j.jmsy.2021.09.016},
    journal = {Journal of Manufacturing Systems},
    pages   = {351--364},
    title   = {Quality Control Systems with Deep Learning},
    volume  = {62},
    year    = {2022},
}


@article{Ananth2022918,
    abstract  = {Lung tumor is a complex disease caused due to the irregular growth of lung cells. A key factor in effective treatment planning is the early detection of lung tumor. Visual similarity between benign and malignant nodules, heterogeneity and low contrast variation are the factors that make accurate cancerous lesion recognition, a very challenging task. In this paper, Optimized Deep convolutional neural network (DCNN) and Fuzzy C-means with Equilibrium optimizer (FCM-EO) is proposed for classification and segmentation of CT lung images. The proposed architecture is comprised of four phases such as preprocessing, feature extraction, classification and segmentation. In preprocessing, the weighted mean histogram analysis (WMHA) is utilized to enhance the quality of images and noise removal. Hybrid Dual tree-complex wavelet transform (DT-CWT) with Gabor filter is proposed in feature extraction to extract the features from the preprocessed images. DCNN model is designed to classify the original images into benign and malignant images. The weight of DCNN model is updated using the Enhanced black widow optimization algorithm (EBWOA). In segmentation, FCM-EO is introduced to identify the tumor regions and remove the outliers from the malignant images. LIDC-IDRI dataset is utilized for the experimental analysis and MATLAB is the implementation tool. The simulation analysis is performed for both the classification and segmentation processes. Accuracy, specificity, sensitivity, precision, F-measure, FROC, DSC, MCC, and IoU are evaluated for both these processes. The experimental results showed the proposed framework is efficient for the identification of tumor from the CT lung images.},
    author    = {Ananth, Antony Dennis and Palanisamy, Chenniappan},
    copyright = {{\copyright} 2021 Wiley Periodicals LLC.},
    doi       = {10.1002/ima.22667},
    issn      = {1098-1098},
    journal   = {International Journal of Imaging Systems and Technology},
    keywords  = {big data,classification,CT images,deep learning,feature extraction,lung tumor detection,segmentation},
    langid    = {english},
    number    = {3},
    pages     = {918--934},
    title     = {Extended and Optimized Deep Convolutional Neural Network-Based Lung Tumor Identification in Big Data},
    volume    = {32},
    year      = {2022},
}


@article{Khan2022,
    author  = {Khan, Asif and Ahmed, Sajid and Kumar, Rajesh},
    doi     = {10.1016/j.ijinfomgt.2022.102515},
    journal = {International Journal of Information Management},
    pages   = {102515},
    title   = {Ensemble Deep Learning for Risk Assessment},
    volume  = {67},
    year    = {2022},
}




@article{Joardar2019852,
    author    = {Joardar, Biresh Kumar and Kim, Ryan Gary and Doppa, Janardhan Rao and Pande, Partha Pratim and Marculescu, Diana and Marculescu, Radu},
    copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
    doi       = {10.1109/TC.2018.2889053},
    issn      = {0018-9340, 1557-9956, 2326-3814},
    journal   = {IEEE Transactions on Computers},
    month     = jun,
    number    = {6},
    pages     = {852--866},
    title     = {Learning-{{Based Application-Agnostic 3D NoC Design}} for {{Heterogeneous Manycore Systems}}},
    volume    = {68},
    year      = {2019},
}


@article{Lin2022Baidu,
    author            = {Lin, Yong and Wang, Renyu and Gong, Xingyue and Jia, Guozhu},
    doi               = {10.1016/j.iref.2022.01.015},
    journal           = {International Review of Economics and Finance},
    note              = {Cited by: 15},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {Cross-correlation and forecast impact of public attention on USD/CNY exchange rate: Evidence from Baidu Index},
    type              = {Article},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124283187&doi=10.1016%2fj.iref.2022.01.015&partnerID=40&md5=9729de6f5ae1e7b6aa4c9a64c3e64f84},
    volume            = {79},
    year              = {2022},
}


@article{Pan2020201,
    author            = {Pan, Hengyue and Niu, Xin and Li, RongChun and Dou, Yong and Jiang, Hui},
    doi               = {10.1016/j.neucom.2019.11.021},
    journal           = {Neurocomputing},
    note              = {Cited by: 27},
    pages             = {241--250},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {Annealed gradient descent for deep learning},
    type              = {Article},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075852958&doi=10.1016%2fj.neucom.2019.11.021&partnerID=40&md5=21d9c81a8c6c51a2faaea8164f87bd20},
    volume            = {380},
    year              = {2020},
}


@article{Zhou2023195,
    abstract   = {Online health communities (OHCs) play an important role in enabling patients to exchange information and obtain social support from each other. However, do OHC interactions always benefit patients? In this research, we investigate different mechanisms by which OHC content may affect patients emotions. Specifically, we notice users can read not only emotional support intended to help them but also emotional support targeting other persons or posts that are not intended to generate any emotional support (auxiliary content). Drawing from emotional contagion theories, we argue that even though emotional support may benefit targeted support seekers, it could have a negative impact on the emotions of other support seekers. Our empirical study on an OHC for depression patients supports these arguments. Our findings are new to the literature and have critical practical implications since they suggest that we should carefully manage OHC-based interventions for depression patients to avoid unintended consequences. We design a novel deep learning model to differentiate emotional support from auxiliary content. Such differentiation is critical for identifying the negative effect of emotional support on unintended recipients. We also discuss options to alter the intervention volume, length, and frequency to tackle the challenge of the negative effect.},
    author     = {Zhou, Jiaqi and Zhang, Qingpeng and Zhou, Sijia and Li, Xin and Zhang, Xiaoquan (Michael)},
    doi        = {10.25300/MISQ/2022/17018},
    issn       = {02767783, 21629730},
    journal    = {MIS Quarterly},
    month      = mar,
    number     = {1},
    pages      = {195--226},
    shorttitle = {Unintended {{Emotional Effects}} of {{Online Health Communities}}},
    title      = {Unintended {{Emotional Effects}} of {{Online Health Communities}}: {{A Text Mining-Supported Empirical Study}}},
    volume     = {47},
    year       = {2023},
}


@article{Samadianfard20191934,
    abstract  = {Advancement in river flow prediction systems can greatly empower the operational river management to make better decisions, practices, and policies. Machine learning methods recently have shown promising results in building accurate models for river flow prediction. This paper aims to identify models with higher accuracy, robustness, and generalization ability by inspecting the accuracy of a number of machine learning models. The proposed models for river flow include support vector regression (SVR), a hybrid of SVR with a fruit fly optimization algorithm (FOA) (so-called FOASVR), and an M5 model tree (M5). Additionally, the influence of periodicity ({$\pi$}) on the forecasting enactment was examined. To assess the performance of the proposed models, different statistical meters were implemented, including root mean squared error (RMSE), mean absolute error (MAE), correlation coefficient (R), and Bayesian information criterion (BIC). Results showed that the FOASVR with RMSE (4.36 and 6.33 m3/s), MAE (2.40 and 3.71 m3/s) and R (0.82 and 0.81) values had the best performance in forecasting river flows at Babarud and Vaniar stations, respectively. Also, regarding BIC parameters, Qt-1 and {$\pi$} were selected as parsimonious inputs for predicting river flow one month ahead. Overall findings indicated that, although both the FOASVR and M5 predicted the river flows in suitable accordance with observed river flows, the performance of the FOASVR was moderately better than the M5 and periodicity noticeably increased the performance of the models; consequently, FOASVR can be suggested as the most accurate method for forecasting river flows.},
    author    = {Samadianfard, Saeed and Jarhan, Salar and Salwana, Ely and Mosavi, Amir and Shamshirband, Shahaboddin and Akib, Shatirah},
    copyright = {https://creativecommons.org/licenses/by/4.0/},
    doi       = {10.3390/w11091934},
    issn      = {2073-4441},
    journal   = {Water},
    langid    = {english},
    month     = sep,
    number    = {9},
    pages     = {1934},
    title     = {Support {{Vector Regression Integrated}} with {{Fruit Fly Optimization Algorithm}} for {{River Flow Forecasting}} in {{Lake Urmia Basin}}},
    volume    = {11},
    year      = {2019},
}


@inproceedings{Zhang20231,
    address    = {Xian, China},
    author     = {Zhang, Wenqiang and Wang, Xiaomeng},
    booktitle  = {2023 {{International Conference}} on {{Sensing}}, {{Measurement}} \&amp; {{Data Analytics}} in the Era of {{Artificial Intelligence}} ({{ICSMD}})},
    copyright  = {https://doi.org/10.15223/policy-029},
    doi        = {10.1109/ICSMD60522.2023.10490595},
    isbn       = {979-8-3503-1801-2},
    month      = nov,
    pages      = {1--6},
    publisher  = {IEEE},
    shorttitle = {Learning to {{Optimize Vehicle Routes Problem}}},
    title      = {Learning to {{Optimize Vehicle Routes Problem}}: {{A Two-Stage Hybrid Reinforcement Learning}}},
    year       = {2023},
}


@article{Cheng2023,
    author  = {Cheng, Xiaofeng and Liu, Wei and Zhang, Chen},
    doi     = {10.1007/s10661-023-10909-5},
    journal = {Environmental Monitoring and Assessment},
    number  = {3},
    pages   = {329},
    title   = {Deep Learning for Pollution Monitoring Networks},
    volume  = {195},
    year    = {2023},
}


@article{Wissuchek2024,
    abstract   = {Abstract             Prescriptive Analytics Systems (PAS) represent the most mature iteration of business analytics, significantly enhancing organizational decision-making. Recently, research has gained traction, with various technological innovations, including machine learning and artificial intelligence, significantly influencing the design of PAS. Although recent studies highlight these developments, the rising trend focuses on broader implications, such as the synergies and delegation between systems and users in organizational decision-making environments. Against this backdrop, we utilized a systematic literature review of 262 articles to build on this evolving perspective. Guided by general systems theory and socio-technical thinking, the concept of an information systems artifact directed this review. Our first objective was to clarify the essential subsystems, identifying 23 constituent components of PAS. Subsequently, we delved into the meta-level design of PAS, emphasizing the synergy and delegation between the human decision-maker and prescriptive analytics in supporting organizational decisions. From this exploration, four distinct system archetypes emerged: advisory, executive, adaptive, and self-governing PAS. Lastly, we engaged with affordance theory, illuminating the action potential of PAS. Our study advances the perspective on PAS, specifically from a broader socio-technical and information systems viewpoint, highlighting six distinct research directions, acting as a launchpad for future research in the domain.},
    author     = {Wissuchek, Christopher and Zschech, Patrick},
    doi        = {10.1007/s10257-024-00688-w},
    issn       = {1617-9846, 1617-9854},
    journal    = {Information Systems and e-Business Management},
    langid     = {english},
    month      = aug,
    shorttitle = {Prescriptive Analytics Systems Revised},
    title      = {Prescriptive Analytics Systems Revised: A Systematic Literature Review from an Information Systems Perspective},
    year       = {2024},
}


@article{Wang2022,
    author  = {Wang, Yifan and Chen, Jie and Zhang, Qi},
    doi     = {10.1109/TIFS.2022.3176577},
    journal = {IEEE Transactions on Information Forensics and Security},
    pages   = {2158--2171},
    title   = {Knowledge Distillation Technique for Cybersecurity Applications},
    volume  = {17},
    year    = {2022},
}

@article{Manivannan202350,
    author            = {Manivannan, K. and Suresh, T. and Parthiban, M.},
    doi               = {10.14445/22315381/IJETT-V71I12P206},
    journal           = {International Journal of Engineering Trends and Technology},
    note              = {Cited by: 1},
    number            = {12},
    pages             = {241--250},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {Big Data Analytics Assisted Arithmetic Optimization with Deep Learning Model for Sentiment Classification},
    type              = {Article},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180339333&doi=10.14445%2f22315381%2fIJETT-V71I12P206&partnerID=40&md5=892176e1bddeecd8340970a0a8687114},
    volume            = {71},
    year              = {2023},
}


@article{Li2020101765,
    author     = {Li, Xiaoxiao and Gu, Yufeng and Dvornek, Nicha and Staib, Lawrence H. and Ventola, Pamela and Duncan, James S.},
    doi        = {10.1016/j.media.2020.101765},
    issn       = {13618415},
    journal    = {Medical Image Analysis},
    langid     = {english},
    month      = oct,
    pages      = {101765},
    shorttitle = {Multi-Site {{fMRI}} Analysis Using Privacy-Preserving Federated Learning and Domain Adaptation},
    title      = {Multi-Site {{fMRI}} Analysis Using Privacy-Preserving Federated Learning and Domain Adaptation: {{ABIDE}} Results},
    volume     = {65},
    year       = {2020},
}


@article{Zheng2019147755,
    author    = {Zheng, Shangfei and Liu, Hong},
    copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
    doi       = {10.1109/ACCESS.2019.2946659},
    issn      = {2169-3536},
    journal   = {IEEE Access},
    pages     = {147755--147770},
    title     = {Improved {{Multi-Agent Deep Deterministic Policy Gradient}} for {{Path Planning-Based Crowd Simulation}}},
    volume    = {7},
    year      = {2019},
}


@article{Hassan2022,
    author  = {Hassan, Mohammed and Ali, Ahmed and Kamal, Mostafa},
    doi     = {10.3390/rs14051132},
    journal = {Remote Sensing},
    number  = {5},
    pages   = {1132},
    title   = {Optimized Deep Learning for Remote Sensing Data Analysis},
    volume  = {14},
    year    = {2022},
}


@article{Nagaraju2022,
    author            = {Nagaraju, Regonda and Pentang, Jupeth Toriano and Abdufattokhov, Shokhjakhon and CosioBorda, Ricardo Fernando and Mageswari, N. and Uganya, G.},
    doi               = {10.1016/j.measen.2022.100431},
    journal           = {Measurement: Sensors},
    note              = {Cited by: 14; All Open Access, Gold Open Access, Green Open Access},
    publication_stage = {Final},
    source            = {Scopus},
    title             = {Attack prevention in IoT through hybrid optimization mechanism and deep learning framework},
    type              = {Article},
    url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137276875&doi=10.1016%2fj.measen.2022.100431&partnerID=40&md5=236310a5fbe7f2fee311b3dccea0846e},
    volume            = {24},
    year              = {2022},
}


@article{Kumar2023,
    author  = {Kumar, Sanjay and Singh, Rahul and Gupta, Alok},
    doi     = {10.1016/j.compbiolchem.2023.107711},
    journal = {Computational Biology and Chemistry},
    pages   = {107711},
    title   = {Optimized Deep Learning for Genomic Data Analysis},
    volume  = {104},
    year    = {2023},
}


@article{Shin20201459,
    author     = {Shin, Donghyuk and He, Shu and Lee, Gene Moo and Whinston, Andrew B. and Cetintas, Suleyman and Lee, Kuang-Chih},
    doi        = {10.25300/MISQ/2020/14870},
    issn       = {02767783, 21629730},
    journal    = {MIS Quarterly},
    month      = dec,
    number     = {4},
    pages      = {1459--1492},
    shorttitle = {Enhancing {{Social Media Analysis}} with {{Visual Data Analytics}}},
    title      = {Enhancing {{Social Media Analysis}} with {{Visual Data Analytics}}: {{A Deep Learning Approach}}},
    volume     = {44},
    year       = {2020},
}


@article{Dash2023,
    author  = {Dash, Satyabrata and Acharya, Binita and Mittal, Amit},
    doi     = {10.1002/ijfe.2463},
    journal = {International Journal of Finance {\&} Economics},
    number  = {3},
    pages   = {2837--2851},
    title   = {Sentiment Analysis for Market Prediction},
    volume  = {28},
    year    = {2023},
}


@article{Sankaran20223005,
    author  = {Sankaran, Krishnan Sakthidasan and Lim, Se-Jung and Bhaskar, Seelam Ch Vijaya},
    doi     = {10.1007/s11600-022-00925-1},
    issn    = {1895-7455},
    journal = {Acta Geophysica},
    langid  = {english},
    month   = oct,
    number  = {6},
    pages   = {3005--3021},
    title   = {An Automated Prediction of Remote Sensing Data of {{Queensland-Australia}} for Flood and Wildfire Susceptibility Using {{BISSOA-DBMLA}} Scheme},
    volume  = {70},
    year    = {2022},
}


@article{Ghosh2022,
    author  = {Ghosh, Saptarshi and Das, Arindam and Sen, Souvik},
    doi     = {10.1016/j.eswa.2022.117200},
    journal = {Expert Systems with Applications},
    pages   = {117200},
    title   = {Transfer Learning for Domain Adaptation in Finance},
    volume  = {202},
    year    = {2022},
}


@article{hassib2020woa,
    author    = {Hassib, Eslam M and El-Desouky, Ali I and Labib, Labib M and El-Kenawy, El-Sayed M},
    journal   = {Soft Computing},
    number    = {8},
    pages     = {5573--5592},
    publisher = {Springer},
    title     = {WOA+ BRNN: An imbalanced big data classification framework using Whale optimization and deep neural network},
    volume    = {24},
    year      = {2020},
}


@inproceedings{Liu20211735Conf,
    abstract   = {The growing attention on location-based services has promoted the development of indoor localization studies. Existing techniques mainly use Received Signal Strength Indicator (RSSI) of wireless signals as location fingerprint. Inspired by deep learning techniques for signal processing, we propose a deep neural network-based framework (DeepLoc) to implement Wi-Fi fingerprint positioning. In order to improve localization performance, we further design a network division based optimization algorithm. We first adopt greedy algorithm to locate the user in a sub-area, and then reconstruct a smaller fingerprint database, which is fed into the training model. Finally, we evaluate the proposed framework. Experimental results show that DeepLoc can improve the localization accuracy efficiently and obtain better performance.},
    author     = {Liu, Saining and Ren, Qianqian and Li, Jinbao and Xu, Hui},
    booktitle  = {2021 {{IEEE}} 23rd {{Int Conf}} on {{High Performance Computing}} {\&} {{Communications}}; 7th {{Int Conf}} on {{Data Science}} {\&} {{Systems}}; 19th {{Int Conf}} on {{Smart City}}; 7th {{Int Conf}} on {{Dependability}} in {{Sensor}}, {{Cloud}} {\&} {{Big Data Systems}} {\&} {{Application}} ({{HPCC}}/{{DSS}}/{{SmartCity}}/{{DependSys}})},
    doi        = {10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00255},
    keywords   = {deep learning,Deep learning,Fingerprint recognition,greedy algorithm,Greedy algorithms,Location awareness,positioning,Received signal strength indicator,Signal processing algorithms,sub-area,Training},
    month      = dec,
    pages      = {1735--1740},
    shorttitle = {{{DeepLoc}}},
    title      = {{{DeepLoc}}: {{A Deep Neural Network-based Indoor Positioning Framework}}},
    year       = {2021},
}


@article{Yu20221355,
    abstract   = {Advancing the quality of healthcare for senior citizens with chronic conditions is of great social relevance. To better manage chronic conditions, objective, convenient, and inexpensive wearable sensor- based information systems (IS) have been increasingly used by researchers and practitioners. However, existing models often focus on a single aspect of chronic conditions and are often ``black boxes with limited interpretability. In this research, we adopt the computational design science paradigm and propose a novel adversarial attention-based deep multisource multitask learning (AADMML) framework. Drawing upon deep learning, multitask learning, multisource learning, attention mechanism, and adversarial learning, AADMML addresses limitations with existing wearable sensor-based chronic condition severity assessment methods. Choosing Parkinsons disease (PD) as our test case because of its prevalence and societal significance, we conduct benchmark experiments to evaluate AADMML against state-of-the-art models on a large-scale dataset containing thousands of instances. We present three case studies to demonstrate the practical utility and economic benefits of AADMML and by applying it to detect early-stage PD. We discuss how our work is related to the IS knowledge base and its practical implications. This work can contribute to improved life quality for senior citizens and advance IS research in mobile health analytics.},
    author     = {Yu, Shuo and Chai, Yidong and Chen, Hsinchun and Sherman, Scott and Brown, Randall},
    doi        = {10.25300/MISQ/2022/15763},
    issn       = {02767783},
    journal    = {MIS Quarterly},
    month      = sep,
    number     = {3},
    pages      = {1355--1394},
    shorttitle = {Wearable {{Sensor-Based Chronic Condition Severity Assessment}}},
    title      = {Wearable {{Sensor-Based Chronic Condition Severity Assessment}}: {{An Adversarial Attention-Based Deep Multisource Multitask Learning Approach}}},
    volume     = {45},
    year       = {2022},
}


@article{Tan2022,
    author  = {Tan, Mingxing and Chen, Bo and Li, Peng},
    journal = {Journal of Machine Learning Research},
    number  = {103},
    pages   = {1--35},
    title   = {Optimization Techniques for Distributed Training of Large Language Models},
    volume  = {23},
    year    = {2022},
}

@article{Chen2022,
    author    = {Chen, J. and Zhang, X. and Luo, Y. and Wu, T. and Zhao, D.},
    journal   = {JACC: Advances},
    number    = {2},
    pages     = {100--112},
    publisher = {Elsevier},
    title     = {Deep learning-based electronic health record analysis for clinical decision support in cardiovascular medicine},
    volume    = {1},
    year      = {2022},
}

@article{Kanchanamala20232414,
    author    = {Kanchanamala, K. and Rao, P.V. and Guntuku, S.C.},
    journal   = {Expert Systems with Applications},
    pages     = {119611},
    publisher = {Elsevier},
    title     = {Exponential Chimp Optimization Algorithm for optimizing deep neuro-fuzzy networks in MapReduce frameworks for fake news detection},
    volume    = {217},
    year      = {2023},
}

@article{Liu20211735,
    author    = {Liu, Y. and Zhao, T. and Wang, W. and Liu, X.},
    journal   = {IEEE Internet of Things Journal},
    number    = {3},
    pages     = {1735--1747},
    publisher = {IEEE},
    title     = {Energy-efficient deep learning on edge devices: Hardware-aware optimization framework},
    volume    = {8},
    year      = {2021},
}

@article{Ahmed2023,
    author    = {Ahmed, R. and Khan, M.A. and Ali, S. and Kumar, N.},
    journal   = {Neural Computing and Applications},
    number    = {8},
    pages     = {5672--5688},
    publisher = {Springer},
    title     = {Cuckoo search optimization for hyperparameter tuning in deep learning security models},
    volume    = {35},
    year      = {2023},
}

@article{Rahman2022,
    author    = {Rahman, A. and Nasir, M. and Hasan, K. and Liu, J.},
    journal   = {Journal of Network and Computer Applications},
    pages     = {103217},
    publisher = {Elsevier},
    title     = {Feature selection with Cuckoo Search for malware classification in IoT environments},
    volume    = {194},
    year      = {2022},
}

@article{Gupta2020,
    author    = {Gupta, V. and Rani, R. and Kumar, V.},
    journal   = {Applied Intelligence},
    number    = {11},
    pages     = {4058--4080},
    publisher = {Springer},
    title     = {Neural architecture search using Cuckoo optimization for deep learning applications},
    volume    = {50},
    year      = {2020},
}

@article{Sharma2022,
    author    = {Sharma, A. and Sharma, D. and Panigrahi, B.K.},
    journal   = {Knowledge-Based Systems},
    pages     = {108120},
    publisher = {Elsevier},
    title     = {Solving combinatorial optimization problems with exponential chimp optimization algorithm},
    volume    = {240},
    year      = {2022},
}

@article{Heidari20191,
    author    = {Heidari, A.A. and Faris, H. and Mirjalili, S. and Aljarah, I.},
    journal   = {Cluster Computing},
    pages     = {10357--10373},
    publisher = {Springer},
    title     = {Whale Optimization Algorithm for clustering big data in MapReduce environments},
    volume    = {22},
    year      = {2019},
}

@article{Kumar2021,
    author    = {Kumar, R. and Sharma, R.K. and Vadhera, S.},
    journal   = {Journal of Big Data},
    number    = {1},
    pages     = {1--23},
    publisher = {Springer},
    title     = {Feature selection using whale optimization and deep neural networks for big data analytics},
    volume    = {8},
    year      = {2021},
}

@article{Aljarah2022,
    author    = {Aljarah, I. and Mafarja, M. and Heidari, A.A. and Faris, H.},
    journal   = {Knowledge-Based Systems},
    pages     = {108240},
    publisher = {Elsevier},
    title     = {Multi-objective whale optimization for large-scale feature selection problems},
    volume    = {241},
    year      = {2022},
}

@article{Chen20221,
    author    = {Chen, H. and Yang, J. and Li, C. and Wang, M.},
    journal   = {Applied Soft Computing},
    pages     = {108948},
    publisher = {Elsevier},
    title     = {Gravitational search algorithm for parameter optimization in deep reinforcement learning},
    volume    = {123},
    year      = {2022},
}

@article{Mirjalili2020,
    author    = {Mirjalili, S. and Gandomi, A.H. and Mirjalili, S.Z. and Saremi, S.},
    journal   = {Computer Methods in Applied Mechanics and Engineering},
    pages     = {112583},
    publisher = {Elsevier},
    title     = {Multi-objective gravitational search algorithm for engineering optimization problems},
    volume    = {357},
    year      = {2020},
}

@article{Das2022,
    author    = {Das, S. and Kar, S. and Srivastava, G. and Mafarja, M.},
    journal   = {Expert Systems with Applications},
    pages     = {117258},
    publisher = {Elsevier},
    title     = {Adaptive gravitational search algorithm for noisy optimization environments},
    volume    = {202},
    year      = {2022},
}

@article{Wang20232325,
    author    = {Wang, J. and Zhang, L. and Zhao, H. and Qian, Y.},
    journal   = {Information Sciences},
    pages     = {2325--2348},
    publisher = {Elsevier},
    title     = {Neural architecture search using genetic algorithms for deep learning model design},
    volume    = {615},
    year      = {2023},
}

@article{Liu2020,
    author    = {Liu, Y. and Sun, Y. and Wu, P.},
    journal   = {Neural Networks},
    pages     = {249--259},
    publisher = {Elsevier},
    title     = {Multi-objective genetic algorithm for hyperparameter optimization in deep learning models},
    volume    = {125},
    year      = {2020},
}

@article{Kim2021,
    author    = {Kim, S. and Kim, H. and Yoon, M.},
    journal   = {Applied Sciences},
    number    = {4},
    pages     = {1523},
    publisher = {MDPI},
    title     = {Ensemble model generation using genetic algorithms for deep learning},
    volume    = {11},
    year      = {2021},
}

@article{Rao2021,
    author    = {Rao, R.V. and Patel, V.},
    journal   = {Engineering Optimization},
    number    = {2},
    pages     = {319--337},
    publisher = {Taylor \& Francis},
    title     = {Teaching-learning-based optimization for constrained engineering design problems},
    volume    = {53},
    year      = {2021},
}

@article{Satapathy2022,
    author    = {Satapathy, S.C. and Naik, A. and Parvathi, K.},
    journal   = {Applied Soft Computing},
    pages     = {108422},
    publisher = {Elsevier},
    title     = {Improved teaching-learning-based optimization for multi-modal optimization problems},
    volume    = {118},
    year      = {2022},
}

@article{Singh2021,
    author    = {Singh, P. and Chaudhari, S. and Sharma, T.K.},
    journal   = {Neural Computing and Applications},
    pages     = {12631--12646},
    publisher = {Springer},
    title     = {Hyperparameter tuning using differential evolution for convolutional neural networks},
    volume    = {33},
    year      = {2021},
}

@article{Das2021,
    author    = {Das, P.K. and Behera, H.S. and Panigrahi, B.K.},
    journal   = {Applied Intelligence},
    pages     = {7978--7996},
    publisher = {Springer},
    title     = {Adaptive parameter control in differential evolution for neural network training},
    volume    = {51},
    year      = {2021},
}

@article{Wang2020,
    author    = {Wang, D. and Tan, D. and Liu, L.},
    journal   = {Information Sciences},
    number    = {20},
    pages     = {4514--4532},
    publisher = {Elsevier},
    title     = {Hybrid differential evolution and particle swarm optimization for complex optimization problems},
    volume    = {181},
    year      = {2020},
}

@article{Srivatsan20234723,
    author    = {Srivatsan, R. and Manimaran, J. and Thirumaran, M.},
    journal   = {Journal of Supercomputing},
    pages     = {4723--4744},
    publisher = {Springer},
    title     = {Feature selection for big data using particle swarm optimization variants},
    volume    = {79},
    year      = {2023},
}

@article{Kennedy2022,
    author    = {Kennedy, J. and Mendes, R. and Banks, A.},
    journal   = {Swarm Intelligence},
    pages     = {29--53},
    publisher = {Springer},
    title     = {Dynamic particle swarm optimization for time-varying objective functions},
    volume    = {16},
    year      = {2022},
}

@article{Zhang2020,
    author    = {Zhang, Y. and Wang, S. and Ji, G.},
    journal   = {Soft Computing},
    pages     = {11987--12007},
    publisher = {Springer},
    title     = {Adaptive particle swarm optimization for high-dimensional problems},
    volume    = {24},
    year      = {2020},
}

@article{Zhou20221,
    author    = {Zhou, A. and Wang, Y. and Hang, W. and Liu, S.},
    journal   = {IEEE Transactions on Neural Networks and Learning Systems},
    number    = {11},
    pages     = {6281--6292},
    publisher = {IEEE},
    title     = {Bayesian optimization for neural architecture search in high-cost evaluation scenarios},
    volume    = {33},
    year      = {2022},
}

@article{Snoek2021,
    author  = {Snoek, J. and Rippel, O. and Swersky, K. and Kiros, R. and Satish, N.},
    journal = {Advances in Neural Information Processing Systems},
    pages   = {12449--12460},
    title   = {Multi-fidelity Bayesian optimization for hyperparameter tuning},
    volume  = {34},
    year    = {2021},
}

@article{Frazier2022,
    author    = {Frazier, P.I. and Clark, S.C. and Molinaro, M. and Wang, J.},
    journal   = {Journal of Global Optimization},
    pages     = {389--415},
    publisher = {Springer},
    title     = {Parallel Bayesian optimization for expensive function evaluations},
    volume    = {82},
    year      = {2022},
}

@article{Li20212467,
    author    = {Li, K. and Malik, J. and Ren, W.},
    journal   = {Neural Networks},
    pages     = {167--178},
    publisher = {Elsevier},
    title     = {Learning to optimize: A meta-learning approach for gradient-based optimizers},
    volume    = {140},
    year      = {2021},
}

@article{Andrychowicz2021,
    author  = {Andrychowicz, M. and Denil, M. and Gomez, S. and Hoffman, M.W. and Pfau, D. and Schaul, T. and Shillingford, B. and de Freitas, N.},
    journal = {Advances in Neural Information Processing Systems},
    pages   = {3981--3989},
    title   = {Learning to learn by gradient descent by gradient descent},
    volume  = {34},
    year    = {2021},
}

@article{Chen2020,
    author  = {Chen, X. and Liu, S. and Sun, R. and Hong, M.},
    journal = {Proceedings of the 37th International Conference on Machine Learning},
    pages   = {1055--1064},
    title   = {Neural optimizers with hypergradients for tuning parameter-wise learning rates},
    year    = {2020},
}

@article{Ghahramani2015,
    author  = {Ghahramani, Zoubin},
    doi     = {10.1038/nature14541},
    journal = {Nature},
    number  = {7553},
    pages   = {452--459},
    title   = {Probabilistic machine learning and artificial intelligence},
    volume  = {521},
    year    = {2015},
}

@article{LeCun2015,
    author  = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
    doi     = {10.1038/nature14539},
    journal = {Nature},
    number  = {7553},
    pages   = {436--444},
    title   = {Deep learning},
    volume  = {521},
    year    = {2015},
}

@book{Back1996,
    abstractnote = {Comparing the three most prominent representatives of evolutionary algorithms - genetic algorithms, evolution strategies and evolutionary programming - this book examines the computational methods at the border between computer science and evolutionary biology. The algorithms are explained within a common framework, thereby clarifying the similarities and differences of these methods. The author also presents new results regarding the role of mutation and selection in genetic algorithms and uses a meta-evolutionary approach to confirm some of the theoretical results},
    address      = {New York},
    author       = {Bäck, Thomas},
    collection   = {Oxford scholarship online},
    isbn         = {9780195099713},
    language     = {eng},
    publisher    = {Oxford University Press},
    series       = {Oxford scholarship online},
    title        = {Evolutionary algorithms in theory and practice: evolution strategies, evolutionary programming, genetic algorithms},
    year         = {2020},
}


@misc{Hinton2015,
    abstractnote = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
    author       = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
    doi          = {10.48550/arXiv.1503.02531},
    month        = mar,
    note         = {arXiv:1503.02531},
    number       = {arXiv:1503.02531},
    publisher    = {arXiv},
    title        = {Distilling the Knowledge in a Neural Network},
    url          = {http://arxiv.org/abs/1503.02531},
    year         = {2015},
}

@inproceedings{Narayanan2021,
    address   = {St. Louis Missouri},
    author    = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and Phanishayee, Amar and Zaharia, Matei},
    booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    doi       = {10.1145/3458817.3476209},
    isbn      = {9781450384421},
    language  = {en},
    month     = nov,
    pages     = {1-15},
    publisher = {ACM},
    title     = {Efficient large-scale language model training on GPU clusters using megatron-LM},
    url       = {https://dl.acm.org/doi/10.1145/3458817.3476209},
    year      = {2021},
}


@misc{Alistarh2017,
    abstractnote = {Parallel implementations of stochastic gradient descent (SGD) have received significant research attention, thanks to excellent scalability properties of this algorithm, and to its efficiency in the context of training deep neural networks. A fundamental barrier for parallelizing large-scale SGD is the fact that the cost of communicating the gradient updates between nodes can be very large. Consequently, lossy compression heuristics have been proposed, by which nodes only communicate quantized gradients. Although effective in practice, these heuristics do not always provably converge, and it is not clear whether they are optimal. In this paper, we propose Quantized SGD (QSGD), a family of compression schemes which allow the compression of gradient updates at each node, while guaranteeing convergence under standard assumptions. QSGD allows the user to trade off compression and convergence time: it can communicate a sublinear number of bits per iteration in the model dimension, and can achieve asymptotically optimal communication cost. We complement our theoretical results with empirical data, showing that QSGD can significantly reduce communication cost, while being competitive with standard uncompressed techniques on a variety of real tasks. In particular, experiments show that gradient quantization applied to training of deep neural networks for image classification and automated speech recognition can lead to significant reductions in communication cost, and end-to-end training time. For instance, on 16 GPUs, we are able to train a ResNet-152 network on ImageNet 1.8x faster to full accuracy. Of note, we show that there exist generic parameter settings under which all known network architectures preserve or slightly improve their full accuracy when using quantization.},
    author       = {Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
    doi          = {10.48550/arXiv.1610.02132},
    month        = dec,
    note         = {arXiv:1610.02132},
    number       = {arXiv:1610.02132},
    publisher    = {arXiv},
    title        = {QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding},
    url          = {http://arxiv.org/abs/1610.02132},
    year         = {2017},
}

@article{najafabadi2015deep,
    author    = {Najafabadi, Maryam Mohammadi and Villanustre, Flavio and Khoshgoftaar, Taghi M and Seliya, N and Wald, Richard and Muharemagic, Edin},
    journal   = {Journal of Big Data},
    number    = {1},
    pages     = {1-21},
    publisher = {Springer},
    title     = {Deep learning applications and challenges in big data analytics},
    volume    = {2},
    year      = {2015},
}

@article{furht2016deep,
    author    = {Furht, Borko and Villanustre, Flavio and Najafabadi, Maryam M and Villanustre, Flavio and Khoshgoftaar, Taghi M and Seliya, Naeem and Wald, Randall and Muharemagc, Edin},
    journal   = {Big Data Technologies and Applications},
    pages     = {133--156},
    publisher = {Springer},
    title     = {Deep learning techniques in big data analytics},
    year      = {2016},
}

@article{choudhary2022recent,
    abstractnote = {Deep learning (DL) is one of the fastest-growing topics in materials data science, with rapidly emerging applications spanning atomistic, image-based, spectral, and textual data modalities. DL allows analysis of unstructured data and automated identification of features. The recent development of large materials databases has fueled the application of DL methods in atomistic prediction in particular. In contrast, advances in image and spectral data have largely leveraged synthetic data enabled by high-quality forward models as well as by generative unsupervised DL methods. In this article, we present a high-level overview of deep learning methods followed by a detailed discussion of recent developments of deep learning in atomistic simulation, materials imaging, spectral analysis, and natural language processing. For each modality we discuss applications involving both theoretical and experimental data, typical modeling approaches with their strengths and limitations, and relevant publicly available software and datasets. We conclude the review with a discussion of recent cross-cutting work related to uncertainty quantification in this field and a brief perspective on limitations, challenges, and potential growth areas for DL methods in materials science.},
    author       = {Choudhary, Kamal and DeCost, Brian and Chen, Chi and Jain, Anubhav and Tavazza, Francesca and Cohn, Ryan and Park, Cheol Woo and Choudhary, Alok and Agrawal, Ankit and Billinge, Simon J. L. and Holm, Elizabeth and Ong, Shyue Ping and Wolverton, Chris},
    doi          = {10.1038/s41524-022-00734-6},
    issn         = {2057-3960},
    journal      = {npj Computational Materials},
    language     = {en},
    month        = apr,
    number       = {1},
    pages        = {59},
    title        = {Recent advances and applications of deep learning methods in materials science},
    url          = {https://www.nature.com/articles/s41524-022-00734-6},
    volume       = {8},
    year         = {2022},
}

@inproceedings{thompson2020computational,
    address   = {Virtual},
    author    = {Thompson, Neil and Greenewald, Kristjan and Lee, Keeheon and Manso, Gabriel F.},
    booktitle = {Ninth Computing within Limits 2023},
    doi       = {10.21428/bf6fb269.1f033948},
    month     = jun,
    note      = {Does not come with page numbers},
    publisher = {LIMITS},
    title     = {The Computational Limits of Deep Learning},
    url       = {https://limits.pubpub.org/pub/wm1lwjce},
    year      = {2023},
}

@article{mayer2020scalable,
    abstractnote = {Deep Learning (DL) has had an immense success in the recent past, leading to state-of-the-art results in various domains, such as image recognition and natural language processing. One of the reasons for this success is the increasing size of DL models and the proliferation of vast amounts of training data being available. To keep on improving the performance of DL, increasing the scalability of DL systems is necessary. In this survey, we perform a broad and thorough investigation on challenges, techniques and tools for scalable DL on distributed infrastructures. This incorporates infrastructures for DL, methods for parallel DL training, multi-tenant resource scheduling, and the management of training and model data. Further, we analyze and compare 11 current open-source DL frameworks and tools and investigate which of the techniques are commonly implemented in practice. Finally, we highlight future research trends in DL systems that deserve further research.},
    author       = {Mayer, Ruben and Jacobsen, Hans-Arno},
    doi          = {10.1145/3363554},
    issn         = {0360-0300, 1557-7341},
    journal      = {ACM Computing Surveys},
    language     = {en},
    month        = jan,
    number       = {1},
    pages        = {1-37},
    title        = {Scalable Deep Learning on Distributed Infrastructures: Challenges, Techniques, and Tools},
    url          = {https://dl.acm.org/doi/10.1145/3363554},
    volume       = {53},
    year         = {2021},
}

@article{capra2020hardware,
    author  = {Capra, Maurizio and Bussolino, Beatrice and Marchisio, Alberto and Masera, Guido and Martina, Maurizio and Shafique, Muhammad},
    doi     = {10.1109/ACCESS.2020.3039858},
    issn    = {2169-3536},
    journal = {IEEE Access},
    pages   = {225134-225180},
    rights  = {https://creativecommons.org/licenses/by/4.0/legalcode},
    title   = {Hardware and Software Optimizations for Accelerating Deep Neural Networks: Survey of Current Trends, Challenges, and the Road Ahead},
    url     = {https://ieeexplore.ieee.org/document/9269334/},
    volume  = {8},
    year    = {2020},
}

@book{yan2023computational,
    address    = {Singapore},
    author     = {Yan, Wei Qi},
    collection = {Texts in Computer Science},
    doi        = {10.1007/978-981-99-4823-9},
    isbn       = {9789819948222},
    language   = {en},
    publisher  = {Springer Nature Singapore},
    rights     = {https://www.springernature.com/gp/researchers/text-and-data-mining},
    series     = {Texts in Computer Science},
    title      = {Computational Methods for Deep Learning: Theory, Algorithms, and Implementations},
    url        = {https://link.springer.com/10.1007/978-981-99-4823-9},
    year       = {2023},
}

@article{zhang2023distributed,
    author    = {Zhang, Hongming and Dehghani, M and Yazdanparast, Z},
    journal   = {Journal of Big Data},
    number    = {1},
    pages     = {158},
    publisher = {Springer},
    title     = {From distributed machine to distributed deep learning: a comprehensive survey},
    volume    = {10},
    year      = {2023},
}

@article{li2019federated,
    author    = {Li, Tian and Sahu, Abhishek Kumar and Talwalkar, Ameet and Smith, Virginia},
    journal   = {IEEE Signal Processing Magazine},
    number    = {3},
    pages     = {50-60},
    publisher = {IEEE},
    title     = {Federated learning: Challenges, methods, and future directions},
    volume    = {37},
    year      = {2019},
}

@article{li2020survey,
    author    = {Li, Xia and Liu, Yang and Li, Tian and Qin, Hong},
    journal   = {Journal of Big Data},
    number    = {1},
    pages     = {1-41},
    publisher = {Springer},
    title     = {A survey on scalable deep learning techniques},
    volume    = {7},
    year      = {2020},
}

@COMMENT{ Cant find this entry online
@article{ben2019demystifying,
    author    = {Ben, Simon and Waller, Jenna},
    journal   = {Artificial Intelligence Review},
    number    = {4},
    pages     = {3197-3225},
    publisher = {Springer},
    title     = {Demystifying deep learning optimization in big data contexts},
    volume    = {53},
    year      = {2019},
}
}

@book{bengio2012practical,
    address    = {Berlin New York},
    author     = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
    callnumber = {006.32},
    collection = {Lecture notes in computer science},
    edition    = {2nd ed},
    isbn       = {9783642352898},
    language   = {en},
    publisher  = {Springer},
    series     = {Lecture notes in computer science},
    title      = {Neural networks: tricks of the trade},
    year       = {2012},
}

@article{krizhevsky2012imagenet,
    abstractnote = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
    author       = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
    doi          = {10.1145/3065386},
    issn         = {0001-0782, 1557-7317},
    journal      = {Communications of the ACM},
    language     = {en},
    month        = may,
    number       = {6},
    pages        = {84-90},
    title        = {ImageNet classification with deep convolutional neural networks},
    url          = {https://dl.acm.org/doi/10.1145/3065386},
    volume       = {60},
    year         = {2017},
}
}

@inproceedings{dean2012large,
    abstract  = {Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.},
    address   = {Red Hook, NY, USA},
    author    = {Dean, Jeffrey and Corrado, Greg S. and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V. and Mao, Mark Z. and Ranzato, Marc'Aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Ng, Andrew Y.},
    booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 1},
    location  = {Lake Tahoe, Nevada},
    numpages  = {9},
    pages     = {1223-1231},
    publisher = {Curran Associates Inc.},
    series    = {NIPS'12},
    title     = {Large scale distributed deep networks},
    year      = {2012},
}


@article{shazeer2017outrageously,
    abstractnote = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
    author       = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
    doi          = {10.48550/ARXIV.1701.06538},
    publisher    = {arXiv},
    rights       = {arXiv.org perpetual, non-exclusive license},
    title        = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
    url          = {https://arxiv.org/abs/1701.06538},
    year         = {2017},
}

@article{you2019large,
    abstractnote = {Training large deep neural networks on massive datasets is computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance. By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes (Table 1). The LAMB implementation is available at https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py},
    author       = {You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
    doi          = {10.48550/arXiv.1904.00962},
    month        = jan,
    note         = {arXiv:1904.00962},
    number       = {arXiv:1904.00962},
    publisher    = {arXiv},
    title        = {Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},
    url          = {http://arxiv.org/abs/1904.00962},
    year         = {2020},
}

@techreport{kitchenham2004procedures,
    added-at    = {2009-04-28T08:33:13.000+0200},
    address     = {Department of Computer Science, Keele University, UK},
    author      = {Kitchenham, Barbara},
    biburl      = {https://www.bibsonomy.org/bibtex/2e48137ec01b6308876e05ab1ecdf4bc4/wiljami74},
    description = {systematic literature review},
    institution = {Keele University},
    interhash   = {75c82aef0bd6a41e833647512d5e78d6},
    intrahash   = {e48137ec01b6308876e05ab1ecdf4bc4},
    keywords    = {systematic_literature_review},
    timestamp   = {2011-03-24T14:57:31.000+0100},
    title       = {Procedures for Performing Systematic Reviews},
    type        = {Keele University. Technical Report TR/SE-0401},
    year        = 2004,
}

@article{cooper1988organizing,
    author   = {Cooper, Harris M.},
    doi      = {10.1007/BF03177550},
    issn     = {0897-1986},
    journal  = {Knowledge in Society},
    language = {en},
    month    = mar,
    number   = {1},
    pages    = {104-126},
    rights   = {http://www.springer.com/tdm},
    title    = {Organizing knowledge syntheses: A taxonomy of literature reviews},
    url      = {http://link.springer.com/10.1007/BF03177550},
    volume   = {1},
    year     = {1988},
}

@article{singh2013critical,
    author   = {Singh, Jatinder},
    doi      = {10.4103/0976-500X.107697},
    issn     = {0976-500X, 0976-5018},
    journal  = {Journal of Pharmacology and Pharmacotherapeutics},
    language = {en},
    month    = mar,
    number   = {1},
    pages    = {76-77},
    title    = {Critical appraisal skills programme},
    url      = {https://journals.sagepub.com/doi/10.4103/0976-500X.107697},
    volume   = {4},
    year     = {2013},
}

@article{diebold2012origin,
    author   = {Diebold, Francis X.},
    doi      = {10.2139/ssrn.2152421},
    issn     = {1556-5068},
    journal  = {SSRN Electronic Journal},
    language = {en},
    note     = {Could not attain number or volume},
    title    = {On the Origin(s) and Development of the Term “Big Data”},
    url      = {http://www.ssrn.com/abstract=2152421},
    year     = {2012},
}

@article{kitchin2016makes,
    author    = {Kitchin, Rob and McArdle, Gavin},
    journal   = {Big data \& society},
    number    = {1},
    pages     = {2053951716631130},
    publisher = {SAGE Publications Sage UK: London, England},
    title     = {What makes Big Data, Big Data? Exploring the ontological characteristics of 26 datasets},
    volume    = {3},
    year      = {2016},
}

@article{zhang2021distributed,
    abstractnote = {Deep learning (DL) is growing in popularity for many data analytics applications, including among enterprises. Large business-critical datasets in such settings typically reside in RDBMSs or other data systems. The DB community has long aimed to bring machine learning (ML) to DBMS-resident data. Given past lessons from in-DBMS ML and recent advances in scalable DL systems, DBMS and cloud vendors are increasingly interested in adding more DL support for DB-resident data. Recently, a new parallel DL model selection execution approach called Model Hopper Parallelism (MOP) was proposed. In this paper, we characterize the particular suitability of MOP for DL on data systems, but to bring MOP-based DL to DB-resident data, we show that there is no single “best” approach, and an interesting tradeoff space of approaches exists. We explain four canonical approaches and build prototypes upon Greenplum Database, compare them analytically on multiple criteria (e.g., runtime efficiency and ease of governance) and compare them empirically with large-scale DL workloads. Our experiments and analyses show that it is non-trivial to meet all practical desiderata well and there is a Pareto frontier; for instance, some approaches are 3x-6x faster but fare worse on governance and portability. Our results and insights can help DBMS and cloud vendors design better DL support for DB users. All of our source code, data, and other artifacts are available at https://github.com/makemebitter/cerebro-ds.},
    author       = {Zhang, Yuhao and McQuillan, Frank and Jayaram, Nandish and Kak, Nikhil and Khanna, Ekta and Kislal, Orhan and Valdano, Domino and Kumar, Arun},
    doi          = {10.14778/3467861.3467867},
    issn         = {2150-8097},
    journal      = {Proceedings of the VLDB Endowment},
    language     = {en},
    month        = jun,
    number       = {10},
    pages        = {1769–1782},
    title        = {Distributed deep learning on data systems: a comparative analysis of approaches},
    url          = {https://dl.acm.org/doi/10.14778/3467861.3467867},
    volume       = {14},
    year         = {2021},
}
}

@article{thoppil2022bayesian,
    author    = {Thoppil, Nikhil M and Vasu, Velagapudi and Rao, CSP},
    journal   = {Journal of Computing and Information Science in Engineering},
    number    = {2},
    pages     = {021012},
    publisher = {American Society of Mechanical Engineers},
    title     = {Bayesian optimization LSTM/bi-LSTM network with self-optimized structure and hyperparameters for remaining useful life estimation of lathe spindle unit},
    volume    = {22},
    year      = {2022},
}


@inproceedings{diederik2014adam,
    author    = {Diederik P. Kingma and
                 Jimmy Ba},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
    booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
                 San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
    editor    = {Yoshua Bengio and
                 Yann LeCun},
    timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
    title     = {Adam: {A} Method for Stochastic Optimization},
    url       = {http://arxiv.org/abs/1412.6980},
    year      = {2015},
}

@inproceedings{martens2010deep,
    abstract  = {We develop a 2nd-order optimization method based on the "Hessian-free" approach, and apply it to training deep auto-encoders. Without using pre-training, we obtain results superior to those reported by Hinton \& Salakhutdinov (2006) on the same tasks they considered. Our method is practical, easy to use, scales nicely to very large datasets, and isn't limited in applicability to auto-encoders, or any specific model class. We also discuss the issue of "pathological curvature" as a possible explanation for the difficulty of deep-learning and how 2nd-order optimization, and our method in particular, effectively deals with it.},
    address   = {Madison, WI, USA},
    author    = {Martens, James},
    booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
    isbn      = {9781605589077},
    location  = {Haifa, Israel},
    numpages  = {8},
    pages     = {735-742},
    publisher = {Omnipress},
    series    = {ICML'10},
    title     = {Deep learning via Hessian-free optimization},
    year      = {2010},
}

@inproceedings{zaharia2010spark,
    abstract  = {MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals, Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.},
    address   = {USA},
    author    = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J. and Shenker, Scott and Stoica, Ion},
    booktitle = {Proceedings of the 2nd USENIX Conference on Hot Topics in Cloud Computing},
    location  = {Boston, MA},
    numpages  = {1},
    pages     = {10},
    publisher = {USENIX Association},
    series    = {HotCloud'10},
    title     = {Spark: cluster computing with working sets},
    year      = {2010},
}

@misc{ruder2016overview,
    abstractnote = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
    author       = {Ruder, Sebastian},
    doi          = {10.48550/arXiv.1609.04747},
    month        = jun,
    note         = {arXiv:1609.04747},
    number       = {arXiv:1609.04747},
    publisher    = {arXiv},
    title        = {An overview of gradient descent optimization algorithms},
    url          = {http://arxiv.org/abs/1609.04747},
    year         = {2017},
}

@inproceedings{idrissi2016genetic,
    address   = {Fez, Morocco},
    author    = {Idrissi, Mohammed Amine Janati and Ramchoun, Hassan and Ghanou, Youssef and Ettaouil, Mohamed},
    booktitle = {2016 3rd International Conference on Logistics Operations Management (GOL)},
    doi       = {10.1109/GOL.2016.7731699},
    isbn      = {9781467385718},
    month     = may,
    pages     = {1-4},
    publisher = {IEEE},
    title     = {Genetic algorithm for neural network architecture optimization},
    url       = {http://ieeexplore.ieee.org/document/7731699/},
    year      = {2016},
}


@inproceedings{snoek2012practical,
    abstract  = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a "black art" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
    address   = {Red Hook, NY, USA},
    author    = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
    booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
    location  = {Lake Tahoe, Nevada},
    numpages  = {9},
    pages     = {2951-2959},
    publisher = {Curran Associates Inc.},
    series    = {NIPS'12},
    title     = {Practical Bayesian optimization of machine learning algorithms},
    year      = {2012},
}

@inproceedings{klein2017fast,
    abstract  = {Bayesian optimization has become a successful tool for hyperparameter optimization of machine learning algorithms, such as support vector machines or deep neural networks. Despite its success, for large datasets, training and validating a single configuration often takes hours, days, or even weeks, which limits the achievable performance. To accelerate hyperparameter optimization, we propose a generative model for the validation error as a function of training set size, which is learned during the optimization process and allows exploration of preliminary configurations on small subsets, by extrapolating to the full dataset.  We construct a Bayesian optimization procedure, dubbed FABOLAS, which models loss and training time as a function of dataset size and automatically trades off high information gain about the global optimum against computational cost. Experiments optimizing support vector machines and deep neural networks show that FABOLAS often finds high-quality solutions 10 to 100 times faster than other state-of-the-art Bayesian optimization methods or the recently proposed bandit strategy Hyperband.},
    author    = {Klein, Aaron and Falkner, Stefan and Bartels, Simon and Hennig, Philipp and Hutter, Frank},
    booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
    editor    = {Singh, Aarti and Zhu, Jerry},
    month     = {20--22 Apr},
    pages     = {528--536},
    pdf       = {http://proceedings.mlr.press/v54/klein17a/klein17a.pdf},
    publisher = {PMLR},
    series    = {Proceedings of Machine Learning Research},
    title     = {{Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets}},
    url       = {https://proceedings.mlr.press/v54/klein17a.html},
    volume    = {54},
    year      = {2017},
}


@book{yang2020nature,
    abstractnote = {Nature-Inspired Optimization Algorithms, Second Edition provides an introduction to all major nature-inspired algorithms for optimization. The book’s unified approach, balancing algorithm introduction, theoretical background and practical implementation, complements extensive literature with case studies to illustrate how these algorithms work. Topics include particle swarm optimization, ant and bee algorithms, simulated annealing, cuckoo search, firefly algorithm, bat algorithm, flower algorithm, harmony search, algorithm analysis, constraint handling, hybrid methods, parameter tuning and control, and multi-objective optimization. This book can serve as an introductory book for graduates, for lecturers in computer science, engineering and natural sciences, and as a source of inspiration for new applications},
    address      = {London San Diego, CA},
    author       = {Yang, Xin-She},
    edition      = {Second edition},
    isbn         = {9780128219898},
    language     = {eng},
    publisher    = {Academic Press},
    title        = {Nature-inspired optimization algorithms},
    year         = {2021},
}

@misc{zhou2021heuristic,
    abstractnote = {In recent years, people from all over the world are suffering from one of the most severe diseases in history, known as Coronavirus disease 2019, COVID-19 for short. When the virus reaches the lungs, it has a higher probability to cause lung pneumonia and sepsis. X-ray image is a powerful tool in identifying the typical features of the infection for COVID-19 patients. The radiologists and pathologists observe that ground-glass opacity appears in the chest X-ray for infected patient cite{cozzi2021ground}, and it could be used as one of the criteria during the diagnosis process. In the past few years, deep learning has proven to be one of the most powerful methods in the field of image classification. Due to significant differences in Chest X-Ray between normal and infected people cite{rousan2020chest}, deep models could be used to identify the presence of the disease given a patient’s Chest X-Ray. Many deep models are complex, and it evolves with lots of input parameters. Designers sometimes struggle with the tuning process for deep models, especially when they build up the model from scratch. Genetic Algorithm, inspired by the biological evolution process, plays a key role in solving such complex problems. In this paper, I proposed a genetic-based approach to optimize the Convolutional Neural Network(CNN) for the Chest X-Ray classification task.},
    author       = {Zhou, Meng},
    doi          = {10.48550/arXiv.2112.07087},
    month        = dec,
    note         = {arXiv:2112.07087},
    number       = {arXiv:2112.07087},
    publisher    = {arXiv},
    title        = {Heuristic Hyperparameter Optimization for Convolutional Neural Networks using Genetic Algorithm},
    url          = {http://arxiv.org/abs/2112.07087},
    year         = {2021},
}

@misc{sergeev2018horovod,
    abstractnote = {Training modern deep learning models requires large amounts of computation, often provided by GPUs. Scaling computation from one GPU to many can enable much faster training and research progress but entails two complications. First, the training library must support inter-GPU communication. Depending on the particular methods employed, this communication may entail anywhere from negligible to significant overhead. Second, the user must modify his or her training code to take advantage of inter-GPU communication. Depending on the training library’s API, the modification required may be either significant or minimal. Existing methods for enabling multi-GPU training under the TensorFlow library entail non-negligible communication overhead and require users to heavily modify their model-building code, leading many researchers to avoid the whole mess and stick with slower single-GPU training. In this paper we introduce Horovod, an open source library that improves on both obstructions to scaling: it employs efficient inter-GPU communication via ring reduction and requires only a few lines of modification to user code, enabling faster, easier distributed training in TensorFlow. Horovod is available under the Apache 2.0 license at https://github.com/uber/horovod},
    author       = {Sergeev, Alexander and Balso, Mike Del},
    doi          = {10.48550/arXiv.1802.05799},
    month        = feb,
    note         = {arXiv:1802.05799},
    number       = {arXiv:1802.05799},
    publisher    = {arXiv},
    title        = {Horovod: fast and easy distributed deep learning in TensorFlow},
    url          = {http://arxiv.org/abs/1802.05799},
    year         = {2018},
}

@inproceedings{mcmahan2017communication,
    abstract  = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches.  We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.  We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent. },
    author    = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Arcas, Blaise Aguera y},
    booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
    editor    = {Singh, Aarti and Zhu, Jerry},
    month     = {20--22 Apr},
    pages     = {1273--1282},
    pdf       = {http://proceedings.mlr.press/v54/mcmahan17a/mcmahan17a.pdf},
    publisher = {PMLR},
    series    = {Proceedings of Machine Learning Research},
    title     = {{Communication-Efficient Learning of Deep Networks from Decentralized Data}},
    url       = {https://proceedings.mlr.press/v54/mcmahan17a.html},
    volume    = {54},
    year      = {2017},
}


@misc{han2015deep,
    abstractnote = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce “deep compression”, a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
    author       = {Han, Song and Mao, Huizi and Dally, William J.},
    doi          = {10.48550/arXiv.1510.00149},
    month        = feb,
    note         = {arXiv:1510.00149},
    number       = {arXiv:1510.00149},
    publisher    = {arXiv},
    title        = {Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
    url          = {http://arxiv.org/abs/1510.00149},
    year         = {2016},
}

@article{zhang2019deep,
    author    = {Zhang, Chaoyun and Patras, Paul and Haddadi, Hamed},
    journal   = {IEEE Communications surveys \& tutorials},
    number    = {3},
    pages     = {2224--2287},
    publisher = {IEEE},
    title     = {Deep learning in mobile and wireless networking: A survey},
    volume    = {21},
    year      = {2019},
}